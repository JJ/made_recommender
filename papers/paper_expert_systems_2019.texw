<<echo=False>>=
import os
import pandas as pd
import scipy.stats  as stats
import matplotlib.pyplot as plt
import sys
sys.path.append(os.getcwd())

from utils import *

FILM_TROPES_JSON_BZ2_FILE = '../datasets/scraper/cache/20190501/films_tropes_20190501.json.bz2'
FILM_EXTENDED_DATASET_BZ2_FILE = '../datasets/extended_dataset.csv.bz2'
USE_HDF = True
SCRAPER_LOG_FILE = '../logs/scrape_tvtropes_20190501_20190512_191015.log'
MAPPER_LOG_FILE = '../logs/map_films_20190526_164459.log'
EVALUATOR_BUILDER_LOG_FILE = '../logs/build_evaluator_20190616_211935.log'
TOP_VALUES = 14
@

\documentclass[APA,LATO1COL]{WileyNJD-v2}
\usepackage{moreverb}

% -- begin: Added by Rubén
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{tabularx}
% -- end: Added by Rubén


\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\articletype{Article Type}%

\received{<day> <Month>, <year>}
\revised{<day> <Month>, <year>}
\accepted{<day> <Month>, <year>}

%\raggedbottom

\begin{document}

\title{Chasing compelling stories using computational intelligence and tropes}

\author[1]{Rubén Héctor García Ortega}

\author[2]{Pablo García Sánchez}

\author[3]{Juan Julián Merelo Guervós}



\address[1]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[2]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[3]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\corres{*Corresponding author name, Corresponding address. \email{authorone@Email.com}}

\presentaddress{Present address}

\abstract[Abstract]{}

\keywords{Computational Intelligence; Tropes; Scrapping; Neural Networks; Genetic Algorithms}

\jnlcitation{\cname{%
\author{Williams K.},
\author{B. Hoskins},
\author{R. Lee},
\author{G. Masato}, and
\author{T. Woollings}} (\cyear{2016}),
\ctitle{A regime analysis of Atlantic winter jet variability applied to evaluate HadGEM3-GC2}, \cjournal{Q.J.R. Meteorol. Soc.}, \cvol{2017;00:1--6}.}

\maketitle

\footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor}


\section{Introduction}\label{sec1}

Crafting quality films is quite challenging, according to ~\cite{Hennig-Thurau2019},
because of the plot complexities and the \textit{multiplicative production function} of entertainment,
in other words, the elements need to work together and a single failing one can provoke a disaster in cascade.
That is why, in the current research, in order to build an authoring tool to help
on the design of films from the quality perspective, we are counting with the experts' contributions
to, both, model the films in a way that describes them from many perspectives, and have a measure of quality
that summarizes all the perspectives in 360 degrees,
considering the \textit{multiplicative production function} as well.
Our candidates for both mechanisms are out there,
tropes or patterns that have been discovered in the films, and the massive human-evaluated ratings, respectively.

% Intenta responder a: que son los tropos y por que nos importan

We can model a film by using the set of tropes that we can find in it.
A trope is as a recurring narrative device~\cite{baldick2015oxford};
it can be a technique, a motif, an archetype or a \textit{clich\'e},
used by the authors to achieve specific effects that might vary from increasing
the interest, surprising, recall familiarity, entertaining, etc,
in their creative works, such as books, films, comics or videogames.
Some tropes are broadly adopted and academically studied such as
the \textit{Three-act Structure} formulated by Syd Field~\cite{field1982screenplay},
the \textit{Hero's Journey} studied by Vogler~\cite{vogler2007writer},
the \textit{McGuffin} popularized by Hitchcock~\cite{truffaut1985hitchcock} and
the \textit{Chekhov's Gun} developed by the Russian writer with the eponymous
name~\cite{bitsilli1983chekhov},
but there are thousands of not-so-widely used tropes as well, discovered and
catalogued everyday by professionals and enthusiastic of the storytelling;
their study is organic, dynamic and extensive.
In this paper, will call \textit{Film DNA} to the set of tropes
that we are able to find in a film
and define the structure, characters, events, mood, settings, narration, etc.
As the tropes are \textit{living concepts},
which grow as they are discovered as common
patterns in other stories, the \textit{Film DNA} is, by definition, incomplete and evolving,
yet it is still interesting to define stories, categorize them
and model them from a mathematical perspective.

% Intenta responder a: Que es la calidad (muy difícil + subjetivo -> rating es un indicador)
At the same time, we need to be able to link a measure of quality to the \textit{Film DNA}
and it is a complex metric.
Luckily we have access to databases with films' information
that includes the popularity of the films and their human evaluated ratings,
provided by the community of fans,
so we can rely on these evaluations as a measure of quality and add it to our model,
in order to process them, make new evaluations and suggestions.

Given this model, that has \textit{Film DNAs} and ratings,
the first thing to confirm is that there are correlations
and internal relations between the \textit{Film DNAs} and the ratings
so we can predict the rating from a \textit{Film DNA}.
Furthermore, even though intuitively
the \textit{Film DNA} is a profound way to describe a story
from many different perspectives, following the analogy of the DNA,
there are environmental factors that could deeply affect the performance of the story as well.
This method might not guarantee the quality of a film,
as a \textit{Film DNA} can lead to infinite implementations/films with different qualities
and there are many parameters to consider
that are not reflected in the \textit{Film DNA}.
however, it can be used as an indicator of the potential of the story or the
most probable implementation based on the universe of currently analyzed films.

The goal of the paper is to demonstrate
how computational intelligence can use film tropes to help on the creation
and improvement of quality films in the context of authoring tools and Content Generation.
% Intenta responder a: Por qué generar o mejorar el conjunto de tropos (sistemas complejos +  guiones + videojuegos, etc)

% Intenta responder a: La metodología (no imagen, solo explicar los pasos de manera general)
Our approach extracts the tropes, the ratings and the \textit{Film DNAs}
from external Data Sources in order to build what we have called the \textit{Extended Dataset},
a set of XXX films that links to XXX tropes and the rating.
We use a Genetic Algorithm to build a \textit{Film DNA}
that maximizes the rating evaluated through a surrogate model:
a \textit{Neural Network} that is trained with the \textit{Extended Dataset}
and is able to predict the rating from the \textit{Film DNA}.

% Intenta explicar: Los experimentos que vamos a hacer
The effect of the different parameters for the Genetic Algorithm
will be tested in two different experiments:
An attempt to find the \textit{Film DNA} with the highest expected rating for fixed set size
and an attempt to \textit{improve} an existing \textit{Film DNA}
by adding tropes that will increase the quality of the film.

% TODO:El resto del paper…


\section{State of the art}
% TODO:Tropes (Citas de papers gente que use tropos para cosas y tb TVTRopes, DBTropes, y las limitaciones). Enlazarlo con lo nuestro.
% TODO:La calidad (Citas de gente que mida la calidad en cualquier cosa, y específicamente estructurales -> tropos). Enlazarlo con lo nuestro.
% TODO: Modelos surrogados. Enlazarlo con lo nuestro.
% TODO: Algoritmos genéticos aplicados a la generación de historias. Enlazarlo con lo nuestro.
% TODO: conclusión: lo que vamos a hacer es mejor

\section{Methodology}

Our methodology is divided in four main steps, explained below and described
in the figure~\ref{fig:main_workflow_extended}:

\begin{itemize}
    \item[Step 1] Extract/scrape the tropes for every film
    and codify them as \textit{Film DNAs}.
    As we will see, our dataset will have limitations derived
    from the fact that it is fed from
    the community, finding that popular films are broadly described in terms of tropes
    and unpopular films poorly described or directly missing.
    \item[Step 2] Map the films' features such as the rating and genres to the previously built
    \textit{Film DNAs}. This \textit{extended dataset}  will show limitations as well
    based on the limitations from the original one
    and the automatic matching based on different heuristics.
    As we will see, A trope that is widely used doesn't need to be linked to good ratings,
    tropes that are present in bad films can become good in different combinations and vice-versa.
    \item[Step 3] Build a Neural Network to predict the rating from a \textit{Film DNA}.
    We will follow different \textit{rules of the thumb}
    to achieve a moderately good solution that serves our purposes.
    The neural network will handle the unknown relations between tropes and their combinations.
    \item[Step 4] Build a Genetic Algorithm with specific operators
    that relies in the Neural Network as surrogate model
    to build new constrained \textit{Film DNAs} that optimize the expected rating.
\end{itemize}

% DONE: Diagrama y explicar los pasos
<<echo=False>>=
films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)


n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())


workflow = f'''
digraph {{
    splines=polyline
    rankdir=LR
    ranksep=0.25;
    margin=0;
    nodesep=0.3;
    graph [ resolution=128, fontsize=30];

    node [margin=0 fontcolor=black fontsize=10 width=1];
    tvtropes[label="TVTropes\nwebsite\n\ntvtropes.org\n " type="database"];
    scrape_tropes[label="Step 1:\nScrape tropes\n\nPython+\nrequests+\nlxml+bz2\n~11.900 pages" type="process"];
    dataset[label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})\n " type="data"];
    imdb[label="IMDB\ndatasets:\nimdb.com/\ninterfaces/" type="database"];
    map_rating[label="Step 2:\nMap Genres\nand Rating\n\nPython+\nHeuristics+\nbz2" type="process"];
    extended_dataset[label="Extended\nDataset\n\nFilm DNA +\nrating" type="data"];
    build_recommender[label="Step 3:\nBuild\nRecommender\n\npandas+\nsklearn\n " type="process"];
    recommender[label="Surrogate model\n\nNeural Network\nFilm DNA ->\nRating\n\nNLPRegressor" type="tool"];
    user[label="User's\npre-selected\ntropes" type="data"];
    dna_builder[label="Step 4:\nGenetic Algorithm\nto Build\nFilm DNAs\n\ninspyred+\ncachetools" type="process"];
    trope_sequence[label="Optimal\nFilm\nDNA" type="data"];

    tvtropes -> scrape_tropes[minlen=0];
    scrape_tropes -> dataset[minlen=1];
    dataset -> map_rating;
    imdb -> map_rating[minlen=0];
    map_rating -> extended_dataset;
    extended_dataset -> build_recommender;
    build_recommender -> recommender;
    recommender -> dna_builder;
    user -> dna_builder[minlen=0];
    dna_builder -> trope_sequence;
}}'''

draw_graphviz(workflow, "main_workflow_extended.pdf")
@

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/main_workflow_extended.pdf}
\caption[Methodology]{Methodology to generate constrained optimal Film DNAs using Genetic Algorithms with Neural
Networks as surrogate models, fed from TVTropes and IMDb.\label{fig:main_workflow_extended}}
\end{figure}


% TODO: Extracción de tropos: Explicar brevemente el tema de TVTRopes, DBTropes y nuetsro scraper + explicar los datos (Gráficas y tops)

\subsection{Step 1: Extract the tropes}

Tropes are described in a live wiki called \cite{tvtropes_2}, that is being
collecting thousand of descriptions and examples of tropes from 2014 until now. As the data is fed by a community
of users, we could find the bias that popular films are better described and analysed in terms of the tropes than independent
films, and that popular tropes are more recognised than very specific ones.
The semantic network of knowledge behind \textit{TVTropes.org} is huge and complex; it massively links hierarchies of tropes
to their usage in creations for digital entertainment. The data, however, is only available through its web interface,
which is why, in order to make it usable by the scientific community, \cite{maltekiesel_2} extracted all
their data to a database so-called \textit{DBTropes.org}.\\

As the base of the research on automatic trope generation, we begun with a dataset based in the
latest version of DBTropes, called PicTropes\cite{garcia2018overview} that included 5,925 films with 18,270 tropes.
However, the last version of BDTropes is from 2016, and the community of users of TVTropes has tripled the size
of the database since then; if we work with the latest data from TVTropes our machine learning algorithms
would benefit from having much entries and hence, provide better results. That's why our first step is to
extract the data directly from TVTropes, as discussed in next section.


<<echo=False>>=
films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)

n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())


workflow = f'''
digraph {{
    splines=polyline
    ranksep=0.3;
    nodesep=0.3;
    rankdir=LR
    margin=0;
    graph [ resolution=128, fontsize=30];

    subgraph cluster_0 {{
        style=invis
        start [label="Start" type="start"]
        extractcategories [label="Extract\nCategories" type="process"];
        extractfilms [label="Extract\nFilms" type="process"];
        extracttropes [label="Extract\nTropes" type="process"];
        dataset1 [label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})" type="data"];
    }}
    subgraph cluster_1 {{
        style=invis
        retrievepage[label="Retrieve\nPage" type="process"];
    }}
    subgraph cluster_2 {{
        style=invis
        filecache[label="File\nCache" type="data"];
        tvtropes[label="TVTropes\nwebsite:\n\nhttps://tvtropes.org/" type="database"];
    }}


    start -> extractcategories -> extractfilms -> extracttropes -> dataset1;
    retrievepage -> filecache;
    retrievepage -> tvtropes;
    extractcategories -> retrievepage[constraint=false, dir="both"];
    extractfilms -> retrievepage[constraint=false, dir="both"];
    extracttropes -> retrievepage[constraint=false, dir="both"];
    fake_start -> fake_1 [style=invis];
    fake_1 -> retrievepage [style=invis];
    fake_start[style=invis]
    fake_1[style=invis]
}}'''

draw_graphviz(workflow, "scraper_workflow.pdf")
@

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/scraper_workflow.pdf}
\caption[Scraper]{Trope scraping process.\label{fig:scraper_workflow}}
\end{figure}

We built a crawler that extracts the tropes from TVtropes:

\begin{itemize}
\item
  The scraper \textbf{extracts all the categories} from the main
  categories page: \\ \href{https://tvtropes.org/pmwiki/pmwiki.php/Main/Film}{https://tvtropes.org/pmwiki/pmwiki.php/Main/Film}.
  Then, for each category page, it extracts \textbf{all the film
  identifiers} assigned to it. Finally, for every film page, it extracts
  \textbf{all the trope identifiers}. As result, it builds a
  \textbf{dictionary of films and tropes}.
\item
  The process \textbf{can be stopped and re-launched at any moment}
  because the pages are permanently stored in the local cache, so it
  will continue from the last page processed.
\item
  The files in cache and the final output file are \textbf{compressed
  using bzip2} with a block size of 900k (the highest compression
  available).
\item
  The file names are \textbf{encoded in base64} to avoid using special
  characters. The character `-' is replaced by '\_'.
\item
  The code avoids slowing down TVTropes servers by \textbf{waiting
  between each download}.
\item
  The execution when no page is cached takes around 3\textasciitilde{}4
  hours. When pages are cached it takes \textasciitilde{}2 minutes. It
  can retrieve around 12K pages.
\end{itemize}

% ----------

<<echo=False>>=
tropes_summary_dictionary = {}
for key in films_dictionary:
    tropes_summary_dictionary[key] = {'tropes':len(films_dictionary[key])}

tropes_summary_dataframe = pd.DataFrame(tropes_summary_dictionary).transpose()

films_summary_dictionary = {}
for key in tropes_dictionary:
    films_summary_dictionary[key] = {'films':len(tropes_dictionary[key])}

films_summary_dataframe = pd.DataFrame(films_summary_dictionary).transpose()
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = tropes_summary_dataframe.plot.hist(log=True, color='#cccccc', figsize=(5, 5.2), ec="k", zorder=2, rwidth=0.5)
plot.set_xlabel("Films by number of tropes")
@
        \caption{}
        \label{fig:histogram_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.sort_values('tropes',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_tropes_films}
    \end{subfigure}
    \caption{\textbf{(a)} Descriptive analysis of the Tropes by appearance in films.
    \textbf{(b)} Histogram of number of tropes by film.
    \textbf{(c)} Top films by number of tropes}
    \label{fig:tropes_analysis}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = films_summary_dataframe.plot.hist(log=True, color='#cccccc', figsize=(5, 5.2), ec="k", zorder=2, rwidth=0.5)
plot.set_xlabel("Tropes by number of films")
@
        \caption{}
        \label{fig:histogram_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.sort_values('films',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_films_tropes}
    \end{subfigure}
    \caption{\textbf{(a)} Descriptive analysis of the films by tropes occurrences.
    \textbf{(b)} Histogram of number of films by tropes.
    \textbf{(c)} Top tropes by number of films}
    \label{fig:films_analysis}
\end{figure}


% TODO: Mapeo con nota: Explicar brevemente el tema de iMDb y el mapeo + explicar los datos y correlaciones (gráficas)
% TODO: Generación del modelo surrogado: Red Neuronal + descripción de parámetros estándar + decisiones + (en proceso de ver lo mínimo que hay que evaluar)
% TODO: Generación de conjuntos de tropos mediante GAs: Descripción del algoritmo + operadores + parámetros

\section{Experimental setup}
% TODO: Básico: Generar conjuntos de tropos de tamaño fijo: Objetivo, con estudio de parámetros (10 ejecuciones al menos), gráfica de evolución / nº de tropos, interpretación de resultados
% TODO: (Opcional) Mejorar un conjunto de tropos (con tropos fijos): IDEM
% TODO: Generar conjuntos de tropos sin tamaño fijo (la película perfecta). Hay películas parecidas o mezcladas?

\section{Conclusions}
% TODO: Objetivo ha sido ...
% TODO: Para ello hemos realizado tal (metodología y experimentos) ...
% TODO: Los resultados nos dicen que ...
% TODO: Esto puede ser la base para una futura linea...

\section{Bibliography}
\bibliography{report}

\end{document}
