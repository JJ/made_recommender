<<echo=False>>=
import os
import pandas as pd
import scipy.stats  as stats
import matplotlib.pyplot as plt
import sys
sys.path.append(os.getcwd())

from utils import *

FILM_TROPES_JSON_BZ2_FILE = '../datasets/scraper/cache/20190501/films_tropes_20190501.json.bz2'
FILM_EXTENDED_DATASET_BZ2_FILE = '../datasets/extended_dataset.csv.bz2'
USE_HDF = True
SCRAPER_LOG_FILE = '../logs/scrape_tvtropes_20190501_20190512_191015.log'
MAPPER_LOG_FILE = '../logs/map_films_20190526_164459.log'
EVALUATOR_BUILDER_LOG_FILE = '../logs/build_evaluator_20190624_223230.log'
TOP_VALUES = 14
EVERYTHING_BUT_TROPES = ['Id','NameTvTropes', 'NameIMDB', 'Rating', 'Votes', 'Year']
EVALUATOR_HYPER_PARAMETERS_LOG_FILE = '../logs/build_evaluator_hyperparameters_20190622_203043.log'
@

\documentclass[APA,LATO1COL]{WileyNJD-v2}
\usepackage{minted}
%\usepackage{moreverb}

% -- begin: Added by Rubén
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{tabularx}
% -- end: Added by Rubén


\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\articletype{Article Type}%

\received{<day> <Month>, <year>}
\revised{<day> <Month>, <year>}
\accepted{<day> <Month>, <year>}

%\raggedbottom

\begin{document}

\title{Chasing compelling stories using computational intelligence and tropes}

\author[1]{Rubén Héctor García Ortega}

\author[2]{Pablo García Sánchez}

\author[3]{Juan Julián Merelo Guervós}



\address[1]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[2]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[3]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\corres{*Corresponding author name, Corresponding address. \email{authorone@Email.com}}

\presentaddress{Present address}

\abstract[Abstract]{}

\keywords{Computational Intelligence; Tropes; Scrapping; Neural Networks; Genetic Algorithms}

%You should probably edit this too - JJ
\jnlcitation{\cname{%
\author{Williams K.},
\author{B. Hoskins},
\author{R. Lee},
\author{G. Masato}, and
\author{T. Woollings}} (\cyear{2016}),
\ctitle{A regime analysis of Atlantic winter jet variability applied to evaluate HadGEM3-GC2}, \cjournal{Q.J.R. Meteorol. Soc.}, \cvol{2017;00:1--6}.}

\maketitle

% Edit or eliminate this - JJ
\footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor}


\section{Introduction}\label{sec1}

Crafting quality films % of fill scripts - jj
 is quite challenging, according to ~\cite{Hennig-Thurau2019},
because of the plot complexities and the
\textit{multiplicative production function} % You should probably define this - JJ
of entertainment,
in other words, the elements need to work together and a single one failing can provoke a disaster in cascade.
That is why, in the current research, and in order to build an authoring tool to help
on the design of films from the quality perspective, we are counting with the
experts' contributions % Contributors are not experts. Maybe talk about the wisdom of crowds - JJ
to both model the films in a way that describes them from many perspectives, and have a measure of quality
that summarizes all the perspectives in 360 degrees,
considering the \textit{multiplicative production function} as well.
Our candidates for both mechanisms are out there,
tropes or patterns that have been discovered in the films, and the massive human-evaluated ratings, respectively.

% Intenta responder a: que son los tropos y por que nos importan

We can model a film % Again, be precise. A bag of tropes does not model a film
% or even a film script. It describes roughly plot devices and atmosphere - JJ
% Maybe you don't want to model, but characterize it. Or maybe describe it- JJ
by using the set of tropes that we can find in it.
A trope is as a recurring narrative device~\cite{baldick2015oxford};
it can be a technique, a motif, an archetype or a \textit{clich\'e},
used by the authors % specify: script writers, producers, director.
% Are we interested in "films in b&w", for instance, which is purely visual? - JJ
 to achieve specific effects that might vary from interest-increasing
to surprising through recall familiarity or entertaining,
in their creative works, such as books, films, comics or videogames.
Some tropes are broadly adopted and academically studied such as
the \textit{Three-act Structure} formulated by Syd Field~\cite{field1982screenplay},
the \textit{Hero's Journey} studied by Vogler~\cite{vogler2007writer},
the \textit{McGuffin} popularized by Hitchcock~\cite{truffaut1985hitchcock} and
the \textit{Chekhov's Gun} developed % or mentioned for the first time? - JJ
by the Russian writer with the eponymous
name~\cite{bitsilli1983chekhov},
but there are thousands of not-so-widely used tropes as well, discovered and
catalogued everyday by professionals and enthusiastic of the storytelling;
their study is organic, dynamic and extensive. % A mention to PicTropes would
% be in order here - JJ
In this paper, we will call \textit{Film DNA} to the set of tropes
that we are able to find in a film
and define the structure, characters, events, mood, settings, narration, etc.
As the tropes are \textit{living concepts},
which grow as they are discovered as common
patterns in other stories, the \textit{Film DNA} is, by definition, incomplete and evolving,
yet it is still interesting to define stories, categorize them
and model them from a mathematical perspective.

% So far we don't know why you are interested in all this, and what's the
% broad outline of the paper. What do you want to do with tropes? The challenges
% should be presented in the first paragraph. You shouldn't arrive at this point
% in the introduction and still wonder what the paper is about - JJ

% Intenta responder a: Que es la calidad (muy difícil + subjetivo -> rating es un indicador)
At the same time, we need to be able to link a measure of quality to the \textit{Film DNA}
and it is a complex metric.
Luckily we have access to databases with films' information
that includes the popularity of the films and their human evaluated ratings,
provided by the community of fans,
so we can rely on these evaluations as a measure of quality and add it to our model,
in order to process them, make new evaluations and suggestions.

Given this model, that includes \textit{Film DNAs} and ratings,
the first thing to confirm is that there are correlations
and internal relations between the \textit{Film DNAs} and the ratings
so we can predict the rating from a \textit{Film DNA}.
Furthermore, even though intuitively
the \textit{Film DNA} is a profound way to describe a story
from many different perspectives, following the analogy of the DNA,
there are environmental factors that could deeply affect the performance of the story as well.
% Those are epigenetic factors, not the DNA. Following the analogy of
% an organism developing from its DNA, maybe... - JJ
This method might not guarantee the quality of a film,
as a \textit{Film DNA} can lead to infinite implementations/films with different qualities
and there are many parameters to consider
that are not reflected in the \textit{Film DNA}.
however, it can be used as an indicator of the potential of the story or the
most probable implementation based on the universe of currently analyzed films.

The goal of the paper is to demonstrate
how computational intelligence can use film tropes to help on the creation
and improvement of quality films % or film scripts? Or film DNA? - JJ
 in the context of authoring tools and Content Generation.
% Intenta responder a: Por qué generar o mejorar el conjunto de tropos (sistemas complejos +  guiones + videojuegos, etc)
% That should be in the first paragraph. "We want to do this, but *this* is
% difficult. So we have thought about solving *that*, which is difficult (but less)
% because... ""

% Intenta responder a: La metodología (no imagen, solo explicar los pasos de manera general)
Our approach extracts the tropes, the ratings and the \textit{Film DNAs}
from external Data Sources in order to build what we have called the \textit{Extended Dataset},
a set of XXX films that links to XXX tropes and the rating.
% You can use variables from the code directly - JJ
We use a genetic algorithm to build a \textit{Film DNA} % trope bag might be better - JJ
that maximizes the rating evaluated through a surrogate model:
a \textit{Neural Network} that is trained with the \textit{Extended Dataset}
and is able to predict the rating from the \textit{Film DNA}.

% «However, submitting our trope bags to the box office is impossible, which is
% why we will be using deep learning to create a surrogate model that is able
% to infer, from any combination of tropes, ratings and box office »
% Write something like that to explain the different steps of what we are going
% to do: first deep learning, then the evolutionary algorithm - JJ

% Intenta explicar: Los experimentos que vamos a hacer
The effect of the different parameters for the Genetic Algorithm
will be tested in two different experiments:
An attempt to find the \textit{Film DNA} with the highest expected rating for fixed set size
and an attempt to \textit{improve} an existing \textit{Film DNA}
by adding tropes that will increase the quality of the film.
% I didn't know about this one. You need to justify this in terms of your objective
% Which is not improving existing films. It might be an academic exercise, but
% you need to justify it very well.

% TODO:El resto del paper…


\section{State of the art}
% But this part is important. You should at least know related work, because
% some of them might be so important to mention in the introduction - JJ
% TODO:Tropes (Citas de papers gente que use tropos para cosas y tb TVTRopes, DBTropes, y las limitaciones). Enlazarlo con lo nuestro.
% TODO:La calidad (Citas de gente que mida la calidad en cualquier cosa, y específicamente estructurales -> tropos). Enlazarlo con lo nuestro.
% TODO: Modelos surrogados. Enlazarlo con lo nuestro.
% TODO: Algoritmos genéticos aplicados a la generación de historias. Enlazarlo con lo nuestro.
% TODO: conclusión: lo que vamos a hacer es mejor

Since the beginning of the century, narrative formalisms have been proposed that allow video game developers and researchers in computational narrative to generate narratives in a procedural manner. For example, the formalism proposed by the work of Propp's \cite{propp2010morphology} (first edited in 1928) is still in use today. It is based on 7 different roles, each with a list of actions they can take over the course of a story. However, it is simply limited to expressiveness, as it is not possible to create new functions. To solve this, other authors propose the use of agents, each with actions and obligations, that allow you to guide them through the arc of a story. This has advantages, as each can be programmed to have different compartments based on psychological models \cite{Thompson18NarrativeEvents}.

Some authors have proposed the use of tropes, defined in the introduction, as a way of structuring a story in a consistent and reusable way. In \cite{Thompson18NarrativeEvents}, a system of agents relies on tropes to obtain a consistent narrative, to describe the social norms that describe the world in which they live. The authors, as in this work, use tropes available on TVTropes as a base and translate them into deontic logic statements, using TropICAL language, which are the input for a logic programming solver. However, they do not use all the tropes, but a small set choosen by hand. %Pablo: Deberia completar esto

However, it is very complicated to evaluate the content generated by an automatic generator, not only because of its non-deterministic behavior that makes it difficult to predict its outputs, but also because of the subjective, diverse and stochastic nature of the users who are going to consume those contents \cite{TogeliusCap4Evaluating}. To evaluate a generator one can use directly the opinion of the designer, or indirectly from human users (for example, from surveys). But another possibility is the use of AI, by simulating and estimating the quality of the content with respect to some metrics. Moreover, through the use of crowdsourcing, it is possible to obtain a large corpus of examples to be used in computational narrative \cite{Guzdial15Crowdsourcing}. In fact, it is possible to extract information about review sites \cite{BoPang08OpinionMining} (such as MetaCritic or IMDB), to be the input of a model like the one we propose in this article.

%%HABLAR DE LOS MODELOS SURROGADOS AQUI

Our previous work is based on some of the ideas mentioned above. In \cite{GarciaOrtega15MADE} we proposed the MADE framework: a multi-agent and parameterizable system that allowed the generation of backstories in massive environments. A genetic algorithm was used to optimize the parameters of the system (simulation time, size of the world, parameters of the behavior of the agents, among others) with respect to the appearance of different archetypes, such as Hero or Villain, among others. These archetypes are defined from the possible actions that an agent can perform: for example, the archetype Villan appears when an agent fights against another for food. However, this form of evaluation was difficult to justify in order to measure the quality of the stories generated, since it was based on an objective decision, such as the number of Heroes and Villains. Later we proposed in \cite{GarciaOrtega16MADE2} a more advanced model, with more complex agents and the possibility of extracting knowledge from a logical reasoner. On this occasion we use as quality metrics the appearance of the archetype of the monomith (or Journey of the Hero) and the different archetypes that compose it. To do this, logical reasoning was applied based on the predicates produced by the different events that emerged in the system. However, as in previous work, the mere appearance of the monomith does not serve to measure the interest of the stories generated by the system.



That is why in this work we propose the combination of the previous ideas: to use crowdsourcing data and film scores to generate a surrogate computational model to evaluate the subjective quality of a story from a set of tropes. %TERMINAR








\section{Methodology}

% The objective of our methodology is... (open source, open science, open data,
% high performance, repeatability... )

Our methodology is divided in four main steps, explained below and described
in the figure~\ref{fig:main_workflow_extended}: % As is usual, say the objective
% you want to achieve and then what you do to achieve that. Explicitly mentioned
% in step 3, check the rest - JJ
\begin{itemize}
    \item[Step 1] Extract/scrape the tropes for every film
    and codify them as \textit{Film DNAs}.
    As we will see, our dataset will have limitations derived
    from the fact that it is fed from
    the community, finding that popular films are broadly described in terms of tropes
    and unpopular films poorly described or directly missing. % Explain variability - JJ
    \item[Step 2] % Disambiguation of film names needs to be at least mentioned - JJ
    % Or rather you should change "map" to disambiguation, which seems the best
    % description for this step - JJ
    Map the films' features such as the rating and genres to the previously built
    \textit{Film DNAs}. This \textit{extended dataset}  will show limitations as well
    based on the original one
    and the automatic matching based on different heuristics.
    As we will see, a trope that is widely used doesn't need to be linked to good ratings,
    tropes that are present in bad films can become good in different combinations and vice-versa.
% Need to explain that this will cull the initial TVtropes database to only a subset - JJ
    \item[Step 3] % Train a surrogate model
    Build a Neural Network to predict the rating from a \textit{Film DNA}.
    We will follow different \textit{rules of thumb}
    to achieve a moderately good solution that serves our purposes.
    The neural network will handle the unknown relations between tropes and their combinations.
    \item[Step 4] % Don't say what you do, say your objective
    % → Optimize the "trope bag" with respect to ratings or box office. - JJ
    Build a Genetic Algorithm with specific operators
    that relies in the Neural Network as surrogate model
    to build new constrained \textit{Film DNAs} that optimize the expected rating.
\end{itemize}

% DONE: Diagrama y explicar los pasos
<<echo=False>>=
films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)


n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())


workflow = f'''
digraph {{
    splines=polyline
    rankdir=LR
    ranksep=0.25;
    margin=0;
    nodesep=0.3;
    graph [ resolution=128, fontsize=30];

    node [margin=0 fontcolor=black fontsize=10 width=1];
    tvtropes[label="TVTropes\nwebsite\n\ntvtropes.org\n " type="database"];
    scrape_tropes[label="Step 1:\nScrape tropes\n\nPython+\nrequests+\nlxml+bz2\n~11.900 pages" type="process"];
    dataset[label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})\n " type="data"];
    imdb[label="IMDB\ndatasets:\nimdb.com/\ninterfaces/" type="database"];
    map_rating[label="Step 2:\nMap Genres\nand Rating\n\nPython+\nHeuristics+\nbz2" type="process"];
    extended_dataset[label="Extended\nDataset\n\nFilm DNA +\nrating" type="data"];
    build_evaluator[label="Step 3:\nBuild\nEvaluator\n\npandas+\nsklearn\n " type="process"];
    evaluator[label="Surrogate model\n\nNeural Network\nFilm DNA ->\nRating\n\nNLPRegressor" type="tool"];
    user[label="User's\npre-selected\ntropes" type="data"];
    dna_builder[label="Step 4:\nGenetic Algorithm\nto Build\nFilm DNAs\n\ninspyred+\ncachetools" type="process"];
    trope_sequence[label="Optimal\nFilm\nDNA" type="data"];

    tvtropes -> scrape_tropes[minlen=0];
    scrape_tropes -> dataset[minlen=1];
    dataset -> map_rating;
    imdb -> map_rating[minlen=0];
    map_rating -> extended_dataset;
    extended_dataset -> build_evaluator;
    build_evaluator -> evaluator;
    evaluator -> dna_builder;
    user -> dna_builder[minlen=0];
    dna_builder -> trope_sequence;
}}'''

draw_graphviz(workflow, "main_workflow_extended.pdf")
@

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/main_workflow_extended.pdf}
\caption[Methodology]{Methodology to generate constrained optimal Film DNAs using Genetic Algorithms with Neural
Networks as surrogate models, fed from TVTropes and IMDb.\label{fig:main_workflow_extended}}
\end{figure}


% TODO: Extracción de tropos: Explicar brevemente el tema de TVTRopes, DBTropes y nuetsro scraper + explicar los datos (Gráficas y tops)

\subsection{Step 1: Extract the tropes}
% Don't need to devote much time to this except if it's extraordinarily novel
% Refer to former report (PicTropes) and the upcoming one - JJ
We are going to use tropes as described in a live wiki called \cite{tvtropes_2}, that is
collecting thousand of descriptions and examples of tropes from 2014 until now. As the data is fed by a community
of users, we could find the bias that popular films are better described and analysed in terms of the tropes than independent
films, and that popular tropes are more recognised than very specific ones. % REFERENCE NEEDED - JJ
% Which means that... (tropes cound be under/overrepresented|false positives and negatives are possible) - JJ
The semantic network of knowledge behind \textit{TVTropes.org} is huge and complex; it massively links hierarchies of tropes
to their usage in creations for digital entertainment. The data, however, is only available through its web interface,
which is why, in order to make it usable by the scientific community, \cite{maltekiesel_2} extracted all
their data to a database so-called \textit{DBTropes.org}.\\
% But we are not using it... And that Db is outdated - JJ
As the base of the research on automatic trope generation, we begun with a dataset based in the
latest version of DBTropes, called PicTropes\cite{garcia2018overview} that included 5,925 films with 18,270 tropes.
However, the last version of BDTropes is from 2016, and the community of users of TVTropes has tripled the size
of the database since then; if we work with the latest data from TVTropes our machine learning algorithms
would benefit from having much entries and hence, provide better results. That's why our first step is to
extract the data directly from TVTropes. % and made this dataset available to the public... - JJ


<<echo=False>>=
films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)

n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())


workflow = f'''
digraph {{
    splines=polyline
    ranksep=0.3;
    nodesep=0.3;
    rankdir=LR
    margin=0;
    graph [ resolution=128, fontsize=30];

    subgraph cluster_0 {{
        style=invis
        start [label="Start" type="start"]
        extractcategories [label="Extract\nCategories" type="process"];
        extractfilms [label="Extract\nFilms" type="process"];
        extracttropes [label="Extract\nTropes" type="process"];
        dataset1 [label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})" type="data"];
    }}
    subgraph cluster_1 {{
        style=invis
        retrievepage[label="Retrieve\nPage" type="process"];
    }}
    subgraph cluster_2 {{
        style=invis
        filecache[label="File\nCache" type="data"];
        tvtropes[label="TVTropes\nwebsite:\n\nhttps://tvtropes.org/" type="database"];
    }}


    start -> extractcategories -> extractfilms -> extracttropes -> dataset1;
    retrievepage -> filecache;
    retrievepage -> tvtropes;
    extractcategories -> retrievepage[constraint=false, dir="both"];
    extractfilms -> retrievepage[constraint=false, dir="both"];
    extracttropes -> retrievepage[constraint=false, dir="both"];
    fake_start -> fake_1 [style=invis];
    fake_1 -> retrievepage [style=invis];
    fake_start[style=invis]
    fake_1[style=invis]
}}'''

draw_graphviz(workflow, "scraper_workflow.pdf")
@

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/scraper_workflow.pdf}
\caption[Scraper]{Trope scraping process.\label{fig:scraper_workflow}}
\end{figure}

% You need to mention it's been released in Pypi and GitHub - JJ
Our scraper extracts all the categories from the main
categories page (\href{https://tvtropes.org/pmwiki/pmwiki.php/Main/Film}{https://tvtropes.org/pmwiki/pmwiki.php/Main/Film})
and for each category page, it extracts all the film identifiers assigned to it.
Finally, for every film page, it extracts all the trope identifiers.
As result, it builds a dictionary of films and tropes.
In order to ease the execution, it can be stopped and re-launched at any time
because it makes use of a local permanent cache compressed in bzip2 with a block size of 900k,
the highest compression available.
% You use like 3 different forms for TvTropes (with or without caps).
% Settle on just 1! - JJ
In order to avoid hurting the TvTropes servers, the process inserts waits between each two consecutive calls.
As result, the executions retrieved around 12K pages in 3 to 4 hours.

<<echo=False>>=
tropes_summary_dictionary = {}
for key in films_dictionary:
    tropes_summary_dictionary[key] = {'tropes':len(films_dictionary[key])}

tropes_summary_dataframe = pd.DataFrame(tropes_summary_dictionary).transpose()

films_summary_dictionary = {}
for key in tropes_dictionary:
    films_summary_dictionary[key] = {'films':len(tropes_dictionary[key])}

films_summary_dataframe = pd.DataFrame(films_summary_dictionary).transpose()
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = tropes_summary_dataframe.plot.hist(log=True, color='#cccccc', figsize=(5, 5.2), ec="k", zorder=2, rwidth=0.5)
plot.set_xlabel("Films by number of tropes")
@
        \caption{}
        \label{fig:histogram_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.sort_values('tropes',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_tropes_films}
    \end{subfigure}
    \caption{\textbf{(a)} Descriptive analysis of the Tropes by appearance in films.
    \textbf{(b)} Histogram of number of tropes by film.
    \textbf{(c)} Top films by number of tropes}
    \label{fig:tropes_analysis}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = films_summary_dataframe.plot.hist(log=True, color='#cccccc', figsize=(5, 5.2), ec="k", zorder=2, rwidth=0.5)
plot.set_xlabel("Tropes by number of films")
@
        \caption{}
        \label{fig:histogram_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.sort_values('films',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_films_tropes}
    \end{subfigure}
    \caption{\textbf{(a)} Descriptive analysis of the tropes by number of films in which they appear.
    \textbf{(b)} Histogram of number of films by tropes. Please note the logarithmic $y$ axis.
    \textbf{(c)} Top tropes by number of films}
    \label{fig:films_analysis}
\end{figure}

% TODO: Explicar los datasets

% DONE: Mapeo con nota: Explicar brevemente el tema de iMDb y el mapeo + explicar los datos y correlaciones (gráficas)

\subsection{Step 2: Map genres and rating}

TVTropes is a huge yet very specific database of tropes but it doesn't include a rating
or links to an external database that we could use as a rating source;
however, IMDb offers their database for non-commercial use and they provide datasets with lots of interesting features,
including the rating and the popularity.
Our research just needs a way to map both tropes and ratings in order to build an extended dataset
that will ultimately help us train a surrogate model.

IMDb Datasets are a compendium of information that IMDb offers for personal and
non-commercial use. Both conditions of use and dataset descriptions are explained in
https://www.imdb.com/interfaces/.

Our current research will make use of these datasets to extend the film information from TVTropes, in particular,
{\tt title.basics.tsv}, which contains metadata from the films such as the title, the year, the genres
and the duration, and {\tt title.ratings.tsv}, which contains the rating and the number of votes.

<<echo=False>>=
films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)

n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())


workflow = f'''
digraph {{
    splines=polyline
    ranksep=0.3;
    nodesep=0.3;
    rankdir=LR
    margin=0;
    graph [ resolution=128, fontsize=30];

    subgraph cluster_0 {{
        style=invis
        start [label="Start" type="start"]
        retrieve [label="Retrieve data" type="process"]
        titles [label="Title/Year\nnormalization" type="process"];
        map_title_year [label="Select\nunique matches\nby title & year" type="process"];
        map_title [label="Select\nunique matches\nby title" type="process"];
        map_popularity [label="Select\nmost popular candidates\nby title" type="process"];
    }}
    subgraph cluster_1 {{
        style=invis
        ignored [label="Ignored films", type="data"]
        extended_dataset [label="Extended Dataset\n\nid,title_imdb,\ntitle_tvtropes,\nyear,genres,\ntropes", type="data"]
    }}
    subgraph cluster_2 {{
        style=invis
        dataset1 [label="Tropes dataset\n\nfilms->tropes\n({n_films}->{n_tropes})" type="data"];
        dataset2 [label="IMDb Dataset\n\ntitle.basics.tsv\n(id,title,\nyear,genres)" type="database"];
        dataset3 [label="IMDb Dataset\n\ntitle.ratings.tsv\n(id,rating,votes)" type="database"];
    }}
    start->retrieve

    retrieve->dataset1[constraint=false];
    retrieve->dataset2[constraint=false];
    retrieve->dataset3[constraint=false];
    dataset1->dataset2[style=invis]
    dataset2->dataset3[style=invis]

    retrieve->titles;
    map_title_year->extended_dataset[constraint=false];
    map_title->extended_dataset[constraint=false];
    map_popularity->extended_dataset[constraint=false];
    map_popularity->ignored;
    titles->map_title_year->map_title->map_popularity;
}}'''

draw_graphviz(workflow, "mapper_workflow.pdf")
@

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/mapper_workflow.pdf}
\caption[Scraper]{Extending the dataset with genres and rating.\label{fig:mapper_workflow}}
\end{figure}

Items in IMDb that don't relate to films are excluded (tvEpisode, tvSeries, tvSpecial, tvShort, videoGame,
tvMiniSeries, titleType) because they are not in our TVTropes scraped dataset and they would only increase
ambiguity as more films might match the same name. % Maybe you're devoting too much time to
% this. I think this should better go to a separate technical report. In a
% journal paper you need to focus on what's innovative, not so much on the mechanics
% - JJ
As described in Figure~\ref{fig:mapper_workflow}, in order to be able to map the film names,
films names are normalized in both cases, TVTropes and IMDb, converting camel-case to Title case, removing
non-alphanumerical values and extra blanks, splitting name and year when required, and converting to lowercase.
Normalized names in TvTropes and IMDb are matched, ideally \{1->1\}.
In order to reduce ambiguity, if the year is present in TvTropes's identifier,
we reduce the search to the specific year in IMDb.
If there are more than one match for a tvTropes's identifier in IMDb,
we just choose the one with the highest number of votes.
This heuristic relies in the fact that both data sources (tropes and votes)
are maintained by different communities of users, enthusiasts in both cases,
so if there is a film in TVTropes and there are many films with the same name in IMDb,
it will probably be the one with highest popularity, that is reflected in the number of votes.
We decided to include the original title so that non-English titles can be handled as well.


<<echo=False>>=
extended_dataframe = read_dataframe(FILM_EXTENDED_DATASET_BZ2_FILE, USE_HDF)
trope_names = [key for key in extended_dataframe.keys() if key not in EVERYTHING_BUT_TROPES and '[GENRE]' not in key]
extended_dataframe['Number of tropes'] = sum(getattr(extended_dataframe,key) for key in trope_names)

coefficient_rating_votes, p_value_rating_votes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Votes'])
coefficient_rating_ntropes, p_value_rating_ntropes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Number of tropes'])
coefficient_votes_ntropes, p_value_votes_ntropes = stats.pearsonr(extended_dataframe['Votes'], extended_dataframe['Number of tropes'])
coefficient_year_ntropes, p_value_year_ntropes = stats.pearsonr(extended_dataframe['Year'], extended_dataframe['Number of tropes'])
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Rating', y='Number of tropes', color='darkgray', label='Films', figsize=(4, 4))
#plot_regression(extended_dataframe, 'Rating', 'Number of tropes', 'black')
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_rating_ntropes%> \\ p-value & <%=p_value_rating_ntropes%> \end{tabularx}}
        \label{fig:rating_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Votes', y='Number of tropes', color='darkgray', label='Films', figsize=(4, 4))
#plot_regression(extended_dataframe, 'Votes', 'Number of tropes', 'black')
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_votes_ntropes%> \\ p-value & <%=p_value_votes_ntropes%> \end{tabularx}}
        \label{fig:votes_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Year', y='Number of tropes', color='darkgray', label='Films', figsize=(4, 4))
#plot_regression(extended_dataframe, 'Year', 'Number of tropes', 'black')
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_year_ntropes%> \\ p-value & <%=p_value_year_ntropes%> \end{tabularx}}
        \label{fig:year_ntropes}
    \end{subfigure}
    \hfill
    \caption{Scatter plot to show the relation between: \textbf{(a)} Rating vs. Nº of tropes
    \textbf{(b)} Votes vs. Nº of tropes \textbf{(c)} Year vs. Nº of tropes}
    \label{fig:extended_dataset_scatterplots}
\end{figure}

A resulting table includes the identifier in IMDb, in TVTropes, the name of the film, the year, the rating,
the number of votes, the tropes for the film (0 if not present, 1 if present) and the genre (0 if not present,
1 if present).

A brief analysis of the new extended dataset,
consisting in <%=extended_dataframe.shape[0]%> films linked to <%=len(trope_names)%> tropes,
shows that there are significant positive correlations between the number of tropes
and the rating, the popularity and the year, as shown in Figure \ref{fig:extended_dataset_scatterplots},
% TODO why?
% Again, this should be mentioned only in conjunction with the paper objectives.
% Does this help understand the surrogate model? Will this help understand the
% eventual GA result? Even if that were the case, spinning initial data analysis
% and scraping mechanics to a technical report will make this paper lighter
% without losing value - JJ
% DONE: Generación del modelo surrogado: Red Neuronal + descripción de parámetros estándar + decisiones + (en proceso de ver lo mínimo que hay que evaluar)

% These steps 1 and 2 are basically the creation of the training set for
% the surrogate model, together with some initial analysis which is not relevant
% to the paper at large. It would be probably much better to publish it separately
% as a technical report in ArXiV - JJ

\subsection{Step 3: Build the Evaluator}

<<echo=False>>=
input, output = get_experiment_execution_information(EVALUATOR_BUILDER_LOG_FILE)
layers = output[output['parameter']=='Layer sizes']['value'][0].split(', ')
@

Given our Extended Dataset, our goal is to build an evaluator
that can be used as a surrogate model by the Genetic Algorithm, in further steps.

The inputs are binary values for every trope defined in the Extended Dataset
and the output is a continuous numeric value that represents the rating of the film.
% Don't use this as a given. It could be preprocessed in many ways. Say
% something along the lines of "the deep learning technique will itself act as
% a feature extractor" or something like that. And in "future work" mention something
% Also mention the fact that most values will be 0 - JJ
A neural network is a technique that perfectly fits in our requirements,
because the relations among inputs are unclear and the effect on the rating is complex.

There are many rules of the thumb to build

We will follow different rules of the thumb to build a proper neural network:
\begin{itemize}
\item Type: Multi-layer perceptron (MLP), a class of
feedforward artificial neural network very flexible and generally used to learn a mapping from inputs to outputs.
\item Hidden layers: 2; according to \cite{reed1999neural},
although a single hidden layer can theoretically be used to approximate any function that we require
(universal approximation theorem),
using two hidden layers (or more) can be much more efficient.
\item Structure: <%='/'.join(layers)%>,
according to the geometric pyramid rule proposed by \cite{masters1993practical}.
\item Activation: ReLU, the most widely used activation function
because of the sparsity and the reduced likelihood of vanishing gradient,
is the default activation function for the hidden layer in the package sklearn.
\item Solver: adam, an efficient algorithm that works well on large problems,
also the default in the package sklearn.
\end{itemize}

<<echo=False>>=
iterations_evaluator = extract_iterations_from_log(log_file_name=EVALUATOR_BUILDER_LOG_FILE)
gridsearch_dataframe = extract_grid_parameters_from_log_and_results(log_file_name=EVALUATOR_HYPER_PARAMETERS_LOG_FILE)
@

\begin{figure}
    \centering
<%=print(get_table_for_dataframe(gridsearch_dataframe))%>
    \caption{Preliminary hyper-parameters' evaluation using 3-folded cross validation}
    \label{fig:gridsearch_results}
\end{figure}

\begin{figure}
    \centering
<<width=".5\\linewidth",echo=False, results='hidden'>>=
plot = iterations_evaluator.plot(x='iteration', y=['loss','validation'], logy=True, color='black', linestyle='-')
plot.lines[1].set_linestyle('--')
leg = plot.legend()
plot.grid()
@
    \caption{Evolution of the loss and the validation score throughout the training iterations}
    \label{fig:evaluator_training}
\end{figure}

Regarding the genres from IMDb, we also consider them (high level) tropes because a genre usually sets
common structures and elements in the film, hence we are including them as features in our MLP.

As we can see in Figure \ref{fig:evaluator_training},
the training of the MLP converges to <%=iterations_evaluator['loss'].iloc[-1]%>.

% Which training? How many did you do? Is this a typical training? - JJ

% DOING: Generación de conjuntos de tropos mediante GAs: Descripción del algoritmo + operadores + parámetros
\subsection{Film DNA builder}


\section{Experimental setup}
% TODO: Básico: Generar conjuntos de tropos de tamaño fijo: Objetivo, con estudio de parámetros (10 ejecuciones al menos), gráfica de evolución / nº de tropos, interpretación de resultados
% TODO: (Opcional) Mejorar un conjunto de tropos (con tropos fijos): IDEM
% TODO: Generar conjuntos de tropos sin tamaño fijo (la película perfecta). Hay películas parecidas o mezcladas?

\section{Results}

\section{Conclusions}
% TODO: Objetivo ha sido ...
% TODO: Para ello hemos realizado tal (metodología y experimentos) ...
% TODO: Los resultados nos dicen que ...
% TODO: Esto puede ser la base para una futura linea...

\section{Bibliography}
\bibliography{report}

\end{document}
