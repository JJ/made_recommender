<<echo=False>>=
import os
import pandas as pd
import scipy.stats  as stats
import matplotlib.pyplot as plt
import sys
sys.path.append(os.getcwd())

from utils import *

FILM_TROPES_JSON_BZ2_FILE = '../datasets/scraper/cache/20190501/films_tropes_20190501.json.bz2'
FILM_EXTENDED_DATASET_TABLE_BZ2_FILE = '../datasets/extended_dataset.csv.bz2'
FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE = '../datasets/extended_dataset.json.bz2'
USE_HDF = True
SCRAPER_LOG_FILE = '../logs/scrape_tvtropes_20190501_20190512_191015.log'
MAPPER_LOG_FILE = '../logs/map_films_20190526_164459.log'
EVALUATOR_BUILDER_LOG_FILE = '../logs/build_evaluator_20190624_223230.log'
TOP_VALUES = 14
EVERYTHING_BUT_TROPES = ['Id','NameTvTropes', 'NameIMDB', 'Rating', 'Votes', 'Year']
EVALUATOR_HYPER_PARAMETERS_LOG_FILE = '../logs/build_evaluator_hyperparameters_20190622_203043.log'


films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)


n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())
@

\documentclass[APA,LATO1COL]{WileyNJD-v2}
\usepackage{minted}
%\usepackage{moreverb}

% -- begin: Added by Rubén
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{tabularx}
\usepackage{ragged2e}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
% -- end: Added by Rubén


\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\articletype{Article Type}%

\received{<day> <Month>, <year>}
\revised{<day> <Month>, <year>}
\accepted{<day> <Month>, <year>}

%\raggedbottom

\begin{document}

\title{Chasing compelling stories using computational intelligence and tropes}

\author[1]{Rubén Héctor García Ortega}

\author[2]{Pablo García Sánchez}

\author[3]{Juan Julián Merelo Guervós}



\address[1]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[2]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[3]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\corres{*Corresponding author name, Corresponding address. \email{authorone@Email.com}}

\presentaddress{Present address}

\abstract[Abstract]{
Designing a plot widely considered a crafty yet critical task that requires deep specific human knowledge
in order to reach a minimum quality and originality.
The present paper aims to propose and evaluate a methodology to automatically synthesise sets of tropes,
usually referred in literature as high level devices used to describe films from different perspectives,
in a way that they maximize the potential rating of a film that conforms to them.
The Computational Intelligence is achieved through a Genetic Algorithm that
relies in a surrogate model, trained with the data extracted and processed from huge film databases in Internet.
In order to evaluate the methodology, we analyse the nature of the tropes and their distributions in existing films,
the performance of the models and the quality of the sets of tropes synthesised.
The results show that the methodology works and we are able to use a Genetic Algorithm that relies in a surrogate model
to build sets of tropes that maximize the rating and that these sets are genuine and novel.
The work has revealed that the methodology and tools developed
are directly suitable for assisting in the plots generation as an authoring tool
and, ultimately, for supporting the automatic generation of stories in massively populated videogames.}

\keywords{Computational Intelligence; Tropes; Scrapping; Neural Networks; Genetic Algorithms}

\jnlcitation{\cname{%
\author{R.H. García-Ortega},
\author{J.J. Merelo},
\author{P. García-Sánchez}} (\cyear{2019}),
\ctitle{Chasing compelling stories using computational intelligence and tropes}, \cjournal{}, \cvol{}.}

\maketitle


\section{Introduction}\label{sec1}

The goal of the paper is to demonstrate
how computational intelligence can use film tropes to help on the creation
and improvement of quality set of tropes
in the context of authoring tools and Content Generation.
Specifically, the paper tries to clarify if it is possible, to use a Genetic Algorithm


Crafting film scripts is quite challenging, according to ~\cite{Hennig-Thurau2019},
because of the plot complexities and the
\textit{multiplicative production function of entertainment},
that promulgate that the elements involved in the development of a media product need to work together
and a single failing one can provoke a disaster in cascade.
That is why, in the current research, and in order to build an authoring tool to help
on the design of the films' plots from the quality perspective, we are counting with
collective contributions,
to both model the films in a way that describes them from different perspectives,
and have a measure of quality
that summarizes all the perspectives in 360 degrees,
considering the \textit{multiplicative production function} as well.
Our candidates for both mechanisms are out there,
tropes or patterns that have been discovered in the films, and the massive human-evaluated ratings, respectively.

We can roughly characterize a film
by using the set of tropes that we can find in it.
A trope is as a recurring narrative device, according to the definition by \cite{baldick2015oxford};
it can be a technique, a motif, an archetype or a \textit{clich\'e},
used by the script writers, producers and directors
to achieve specific effects that might vary from interest-increasing
to surprising through recall familiarity or entertaining,
in their creative works, such as books, films, comics or videogames.
Some tropes are broadly adopted and academically studied such as
the \textit{Three-act Structure} formulated by \cite{field1982screenplay},
the \textit{Hero's Journey} studied by \cite{vogler2007writer},
the \textit{McGuffin} popularized by Hitchcock, according to \cite{truffaut1985hitchcock}, and
the \textit{Chekhov's Gun} formulated by the Russian writer with the eponymous
name, according to \cite{bitsilli1983chekhov};
however, there are thousands of not-so-widely used tropes as well, discovered and
catalogued everyday by professionals and enthusiastic of the storytelling;
their study is organic, dynamic and extensive, according to~\cite{garcia2018overview}.


Along this paper, we use the analogy of the \textit{Film DNA}
and define it as the set of tropes that are found in a film
and define its structure, characters, events, mood, settings, narration, etc.
As the tropes are \textit{living concepts},
which grow as they are discovered as common
patterns in other stories, the \textit{Film DNA} is, by definition, incomplete and evolving,
yet it is still interesting to define stories, categorize them
and model them from a mathematical perspective.
The challenge of our research if to build original synthetic Film DNAs
based on a huge corpus of film-tropes, and through computational intelligence,
in a way that they have an intrinsic potential quality when reflected together in a film.

At the same time, we need to be able to associate a measure of quality to the \textit{Film DNA}
and it is a complex metric that summarizes many factors, at last, perceived and evaluated by humans.
Luckily we have access to databases with films' information
that includes the genres of the films and their human evaluated ratings,
provided by the community of fans; we add this information to our knwoledge base
to build a \textit{Extended Dataset}, that includes \textit{Film DNAs}, their genres and their rating,
and we rely on the ratings as a measure of quality and add it to our model,
in order to process them, make new evaluations and suggestions.
Furthermore, even though intuitively
the \textit{Film DNA} is a profound way to describe a story
from many different perspectives, following the analogy of the DNA,
there are epigenetic factors that could deeply affect the performance of the story as well.
This method does not guarantee the quality of a film,
as a film that develops from a \textit{Film DNA} can implement them in infinite ways with very different
results in terms of quality;
however, it can be used as an indicator of the \textit{potential of the story} or the
most probable implementation based on the universe of currently analyzed films.

Our approach extracts <%=n_films%> \textit{Film DNAs} that contain in sum <%=n_tropes%> different tropes
from external Data Sources
and maps it to a database of film ratings and genres, dealing with disambiguation heuristics
in order to build what we have called the \textit{Extended Dataset}.
However, submitting a set of tropes to the box office is impossible, which is
why we use deep learning to create a surrogate model that is able
to infer the rating from any combination of tropes.
We perform different analysis in order to determine the quality
of the predictions and the parameters that could affect them.
Later on, a Genetic Algorithm and their operators are defined in a way
that the trope combinations, formerly Film DNAs, are evolved relying in the surrogate model to
maximize the rating.

The remaining of this work is organized as follows:
in the Section~\ref{sec:state_of_the_art} we explore the current state-of-the-art
in plot generation based on tropes, in the Section~\ref{sec:methodology} we deepen the methodology presented above,
in the Section~\ref{sec:experimental_setup} we describe the experiments carried out to evaluate the methodology,
in the Section~\ref{sec:discussion} we discuss the results
and in the Section~\ref{sec:discussion} we summarize the outcomes and future work.

\section{State of the art} \label{sec:state_of_the_art}



% Formalismos narrativos y gramáticas

Some of the narrative formalisms that allow film-makers, videogame developers and researchers in computational narrative
generate stories in a procedural manner have been proposed almost a century ago,
for example, the one suggested by \cite{propp2010morphology}, first edited in 1928 and still in use today,
is based on seven different roles, each with a list of actions that can take over the course of a story,
in a fixed sequence of 31 functions; however, it is simply limited to expressiveness,
as it is not possible to create new functions.
% revisits Propp’s morphology to build a system that generates instances of Russian folk tale (http://drops.dagstuhl.de/opus/volltexte/2013/4156/)
% y otro que tb usa propp grammar https://ieeexplore.ieee.org/abstract/document/6505320
% y otro https://ieeexplore.ieee.org/abstract/document/6138227
% tb hay un poyo que usa gramáticas, pero es más que interesante https://ieeexplore.ieee.org/abstract/document/5585934

Como ceñirnos a gramáticas de Propp es demasiado limitante,
other authors propose the use of agents, each with actions and obligations,
that allow to guide them through the arc of a story.
This has advantages, as each agent can be programmed to have different behaviours based on psychological models,
according to \cite{Thompson18NarrativeEvents}.


Aunque la aproximación basada en agentes es muy interesante, consideramos que la idea de coger
Muchos artículos se centran en el uso de de Propp's grammars, y está bien, para nosotros lo interesante
es que basarse en Propp grammars es utilizar patrones, y lo que buscamos precisamente. Nuestra filosofía en esta investigación
no será bajar a la granularidad de la narración, sino a una generalidad de patrones que permitan definir un guión a diferentes niveles
mediante patrones, como el de Propp.


Some authors have proposed the use of tropes, defined in the introduction,
as a way of structuring a story in a consistent and reusable way.
In the work of \cite{Thompson18NarrativeEvents}, a system of agents relies on tropes to obtain a consistent narrative,
to describe the social norms that model the world in which they live.
The authors, as in this work, use tropes available on TVTropes as a base
and translate them into logic statements that express duty or obligation,
using TropICAL language, which are the input for a logic programming solver;
however, they do not use all the tropes,
but a small set chosen by hand.

% Otros autores tb han usado tropos, como por ejemplo Hablar de GHOST: a GHOst STory-writer

% sin embargo, nuestra propuesta implica evitar la selección manual de los tropos y confiar en Hablar de %smarter than even individual experts, according to AAA.%James Surowiecki in his 2004 book, The Wisdom of Crowds

El tema es que es muy dificil evaluar una historia, mucho mñas dificil es evaluar un conjunto de tropos que caracterizan una historia

Nevertheless, it is very complicated to evaluate the content generated by an automatic generator,
not only because of its non-deterministic behaviour that makes it difficult to predict its outputs,
but also because of the subjective, diverse and stochastic nature of the audience,
as stated by \cite{TogeliusCap4Evaluating}.

To evaluate a generator one can use directly the opinion of the designer,
or indirectly from the audience, for example, using surveys. % por ejemplo https://ieeexplore.ieee.org/abstract/document/5585934
But another possibility is the use of Artificial Intelligence,
by simulating and estimating the quality of the content via some metrics.
Moreover, through the use of user-generated data, it is possible to obtain a large corpus
of examples to be used in computational narrative, as in th work of \cite{Guzdial15Crowdsourcing}.
In fact, it is possible to extract information about review
sites, according to \cite{BoPang08OpinionMining}, such as MetaCritic or IMDb,
to be the input of a model like the one we propose in this article.

% buscar si hay referencias sobre que el uso de tropos ya implica interés

%%HABLAR DE LOS MODELOS SURROGADOS AQUI

% alguien los usa?
%alguien los usa para medir la calidad?
%alguien usa neural networks?

% alguien usa GAs para modelar plots o plot devices?

% regular grammar is developed to model various causal relationships inside a given story world. The grammar is then evolved using evolutionary computation techniques to generate novel story plots, i.e. story-based scenarios. (https://ieeexplore.ieee.org/abstract/document/5585934)
% Otra propone tree adjoining grammar (TAG) to capture semantics of a story with long-distance causal dependency --- using grammar guided genetic programming (GGGP) https://link.springer.com/chapter/10.1007/978-3-642-17298-4_14


Our previous work is based on some of the ideas mentioned above.
In the work by \cite{GarciaOrtega15MADE}, we proposed the MADE framework:
a parametrizable multi-agent system that allowed
the generation of backstories in massive environments.
A genetic algorithm was used to optimize the parameters of the system, for instance the simulation time,
the size of the world and the parameters of the behaviour of the agents,
with respect to the appearance of different archetypes, such as the {\em Hero} or the {\em Villain}.
These archetypes are defined by the possible actions that an agent can perform:
for example, the archetype Villain appears when an agent fights against another for food;
however, this form of evaluation was difficult to justify in order to measure the quality of the generated stories,
since it was based on an objective decision:
just the number of different character archetypes that emerge during the run of the world.
Also, the list of possible actions for the agents was very limited.
That is the reason that, later on, we proposed in the work by \cite{GarciaOrtega16MADE2},
a more advanced model, with more complex agents and the possibility of extracting knowledge from a logical reasoner.
On this occasion we used as quality metric the appearance of the archetype of the Monomyth, also known as The Hero's Journey,
and the different archetypes that compose it.
In order to do this, logical reasoning was applied based on the predicates produced by the different events
that emerged in the system; however, as in previous work, the mere appearance of the monomyth does not fully serve
as a measure the interest of the stories generated by the system.

That is why in this work we propose the combination of the previous ideas:
% usar tropos como medida de elementos interesantes
% confiar en el wisdom of the crowd de tvtropes
% confiar en la nota de IMDb
% confiar en modelos surrogados para la evaluación, específicamente, redes neuronales
% confiar en un GA como modelo para la generación de tropos
to use user-generated data and film scores to build a surrogate computational model
and use it to evaluate the subjective quality of a story from a set of tropes.

% TODO: conclusión: lo que vamos a hacer es mejor

\section{Methodology} \label{sec:methodology}

% The objective of our methodology is... (open source, open science, open data,
% high performance, repeatability... )

Our methodology is divided in four main steps, explained below and described
in the figure~\ref{fig:main_workflow_extended}: % As is usual, say the objective
% you want to achieve and then what you do to achieve that. Explicitly mentioned
% in step 3, check the rest - JJ
\begin{itemize}
    \item[Step 1] Extract/scrape the tropes for every film
    and codify them as \textit{Film DNAs}.
    As we will see, our dataset will have limitations derived
    from the fact that is fed from
    the community, finding that popular films are broadly described in terms of tropes
    and unpopular films poorly described or directly missing. % Explain variability - JJ
    \item[Step 2] % Disambiguation of film names needs to be at least mentioned - JJ
    % Or rather you should change "map" to disambiguation, which seems the best
    % description for this step - JJ
    Map the films' features such as the rating and genres to the previously built
    \textit{Film DNAs}. This \textit{extended dataset}  will show limitations as well
    based on the original one
    and the automatic matching based on different heuristics.
    As we will see, a trope that is widely used doesn't need to be linked to good ratings,
    tropes that are present in bad films can become good in different combinations and vice-versa.
% Need to explain that this will cull the initial TVtropes database to only a subset - JJ
    \item[Step 3] % Train a surrogate model
    Build a Neural Network to predict the rating from a \textit{Film DNA}.
    We will follow different \textit{rules of thumb}
    to achieve a moderately good solution that serves our purposes.
    The neural network will handle the unknown relations between tropes and their combinations.
    \item[Step 4] % Don't say what you do, say your objective
    % → Optimize the "trope bag" with respect to ratings or box office. - JJ
    Build a Genetic Algorithm with specific operators
    that relies in the Neural Network as surrogate model
    to build new constrained \textit{Film DNAs} that optimize the expected rating.
\end{itemize}

% DONE: Diagrama y explicar los pasos
<<echo=False>>=
workflow = f'''
digraph {{
    splines=polyline
    rankdir=LR
    ranksep=0.25;
    margin=0;
    nodesep=0.3;
    graph [ resolution=128, fontsize=30];

    node [margin=0 fontcolor=black fontsize=10 width=1];
    tvtropes[label="TVTropes\nwebsite\n\ntvtropes.org\n " type="database"];
    scrape_tropes[label="Step 1:\nScrape tropes\n\nPython+\nrequests+\nlxml+bz2\n~11.900 pages" type="process"];
    dataset[label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})\n " type="data"];
    imdb[label="IMDB\ndatasets:\nimdb.com/\ninterfaces/" type="database"];
    map_rating[label="Step 2:\nMap Genres\nand Rating\n\nPython+\nHeuristics+\nbz2" type="process"];
    extended_dataset[label="Extended\nDataset\n\nFilm DNA +\nrating" type="data"];
    build_evaluator[label="Step 3:\nBuild\nEvaluator\n\npandas+\nsklearn\n " type="process"];
    evaluator[label="Surrogate model\n\nNeural Network\nFilm DNA ->\nRating\n\nNLPRegressor" type="tool"];
    user[label="User's\npre-selected\ntropes" type="data"];
    dna_builder[label="Step 4:\nGenetic Algorithm\nto Build\nFilm DNAs\n\ninspyred+\ncachetools" type="process"];
    trope_sequence[label="Optimal\nFilm\nDNA" type="data"];

    tvtropes -> scrape_tropes[minlen=0];
    scrape_tropes -> dataset[minlen=1];
    dataset -> map_rating;
    imdb -> map_rating[minlen=0];
    map_rating -> extended_dataset;
    extended_dataset -> build_evaluator;
    build_evaluator -> evaluator;
    evaluator -> dna_builder;
    user -> dna_builder[minlen=0];
    dna_builder -> trope_sequence;
}}'''

draw_graphviz(workflow, "main_workflow_extended.pdf")
@

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/main_workflow_extended.pdf}
\caption[Methodology]{Methodology to generate constrained optimal Film DNAs using Genetic Algorithms with Neural
Networks as surrogate models, fed from TVTropes and IMDb.\label{fig:main_workflow_extended}}
\end{figure}


% TODO: Extracción de tropos: Explicar brevemente el tema de TVTRopes, DBTropes y nuetsro scraper + explicar los datos (Gráficas y tops)

\subsection{Step 1: Extract the tropes} \label{sec:methodology_scraper}
% Don't need to devote much time to this except if it's extraordinarily novel
% Refer to former report (PicTropes) and the upcoming one - JJ
We are going to use tropes as described in a live wiki called \cite{tvtropes_2}, that is
collecting thousand of descriptions and examples of tropes from 2014 until now. As the data is fed by a community
of users, we could find the bias that popular films are better described and analysed in terms of the tropes than independent
films, and that popular tropes are more recognised than very specific ones. % REFERENCE NEEDED - JJ
% Which means that... (tropes cound be under/overrepresented|false positives and negatives are possible) - JJ
The semantic network of knowledge behind \textit{TVTropes.org} is huge and complex; it massively links hierarchies of tropes
to their usage in creations for digital entertainment. The data, however, is only available through its web interface,
which is why, in order to make it usable by the scientific community, \cite{maltekiesel_2} extracted all
their data to a database so-called \textit{DBTropes.org}.\\
% But we are not using it... And that Db is outdated - JJ
As the base of the research on automatic trope generation, we begun with a dataset based in the
latest version of DBTropes, called PicTropes\cite{garcia2018overview} that included 5,925 films with 18,270 tropes.
However, the last version of BDTropes is from 2016, and the community of users of TVTropes has tripled the size
of the database since then; if we work with the latest data from TVTropes our machine learning algorithms
would benefit from having much entries and hence, provide better results. That's why our first step is to
extract the data directly from TVTropes. % and made this dataset available to the public... - JJ


<<echo=False>>=
films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)

n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())


workflow = f'''
digraph {{
    splines=polyline
    ranksep=0.3;
    nodesep=0.3;
    rankdir=LR
    margin=0;
    graph [ resolution=128, fontsize=30];

    subgraph cluster_0 {{
        style=invis
        start [label="Start" type="start"]
        extractcategories [label="Extract\nCategories" type="process"];
        extractfilms [label="Extract\nFilms" type="process"];
        extracttropes [label="Extract\nTropes" type="process"];
        dataset1 [label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})" type="data"];
    }}
    subgraph cluster_1 {{
        style=invis
        retrievepage[label="Retrieve\nPage" type="process"];
    }}
    subgraph cluster_2 {{
        style=invis
        filecache[label="File\nCache" type="data"];
        tvtropes[label="TVTropes\nwebsite:\n\nhttps://tvtropes.org/" type="database"];
    }}


    start -> extractcategories -> extractfilms -> extracttropes -> dataset1;
    retrievepage -> filecache;
    retrievepage -> tvtropes;
    extractcategories -> retrievepage[constraint=false, dir="both"];
    extractfilms -> retrievepage[constraint=false, dir="both"];
    extracttropes -> retrievepage[constraint=false, dir="both"];
    fake_start -> fake_1 [style=invis];
    fake_1 -> retrievepage [style=invis];
    fake_start[style=invis]
    fake_1[style=invis]
}}'''

draw_graphviz(workflow, "scraper_workflow.pdf")
@

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/scraper_workflow.pdf}
\caption[Scraper]{Trope scraping process.\label{fig:scraper_workflow}}
\end{figure}

% You need to mention it's been released in Pypi and GitHub - JJ
Our scraper extracts all the categories from the main
categories page (\href{https://tvtropes.org/pmwiki/pmwiki.php/Main/Film}{https://tvtropes.org/pmwiki/pmwiki.php/Main/Film})
and for each category page, it extracts all the film identifiers assigned to it.
Finally, for every film page, it extracts all the trope identifiers.
As result, it builds a dictionary of films and tropes.
In order to ease the execution, it can be stopped and re-launched at any time
because it makes use of a local permanent cache compressed in bzip2 with a block size of 900k,
the highest compression available.
% You use like 3 different forms for TvTropes (with or without caps).
% Settle on just 1! - JJ
In order to avoid hurting the TvTropes servers, the process inserts waits between each two consecutive calls.
As result, the executions retrieved around 12K pages in 3 to 4 hours.

<<echo=False>>=
tropes_summary_dictionary = {}
for key in films_dictionary:
    tropes_summary_dictionary[key] = {'tropes':len(films_dictionary[key])}

tropes_summary_dataframe = pd.DataFrame(tropes_summary_dictionary).transpose()

films_summary_dictionary = {}
for key in tropes_dictionary:
    films_summary_dictionary[key] = {'films':len(tropes_dictionary[key])}

films_summary_dataframe = pd.DataFrame(films_summary_dictionary).transpose()
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = tropes_summary_dataframe.plot.hist(log=True, color='#cccccc', figsize=(5, 5.2), ec="k", zorder=2, rwidth=0.5)
plot.set_xlabel("Films by number of tropes")
@
        \caption{}
        \label{fig:histogram_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.sort_values('tropes',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_tropes_films}
    \end{subfigure}
    \caption{\textbf{(a)} Descriptive analysis of the Tropes by appearance in films.
    \textbf{(b)} Histogram of number of tropes by film.
    \textbf{(c)} Top films by number of tropes}
    \label{fig:tropes_analysis}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = films_summary_dataframe.plot.hist(log=True, color='#cccccc', figsize=(5, 5.2), ec="k", zorder=2, rwidth=0.5)
plot.set_xlabel("Tropes by number of films")
@
        \caption{}
        \label{fig:histogram_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.sort_values('films',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_films_tropes}
    \end{subfigure}
    \caption{\textbf{(a)} Descriptive analysis of the tropes by number of films in which they appear.
    \textbf{(b)} Histogram of number of films by tropes. Please note the logarithmic $y$ axis.
    \textbf{(c)} Top tropes by number of films}
    \label{fig:films_analysis}
\end{figure}

% TODO: Explicar los datasets

% DONE: Mapeo con nota: Explicar brevemente el tema de iMDb y el mapeo + explicar los datos y correlaciones (gráficas)

\subsection{Step 2: Map genres and rating} \label{sec:methodology_mapper}

TVTropes is a huge yet very specific database of tropes but it doesn't include a rating
or links to an external database that we could use as a rating source;
on the other hand, IMDb offers their database for non-commercial use and they provide datasets with lots of interesting features,
including the rating and the popularity. % What's the difference? - JJ
Our research just needs a way to map both tropes and ratings in order to build an extended dataset
that will ultimately help us train a surrogate model. % Not clear. You want to identify a movie in TVTropes with another in IMdb. Just say so - JJ

IMDb Datasets are a compendium of information that IMDb offers for personal and
non-commercial use. Both conditions of use and dataset descriptions are explained in
https://www.imdb.com/interfaces/. % Best as a reference. - JJ

Our current research will make use of these datasets to extend the film information from TVTropes, in particular,
{\tt title.basics.tsv}, % filename is unneeded. Use dataset name - JJ
 which contains metadata from the films such as the title, the year, the genres
and the duration, and {\tt title.ratings.tsv}, which contains the rating and the number of votes.

<<echo=False>>=
workflow = f'''
digraph {{
    splines=polyline
    ranksep=0.3;
    nodesep=0.3;
    rankdir=LR
    margin=0;
    graph [ resolution=128, fontsize=30];

    subgraph cluster_0 {{
        style=invis
        start [label="Start" type="start"]
        retrieve [label="Retrieve data" type="process"]
        titles [label="Title/Year\nnormalization" type="process"];
        map_title_year [label="Select\nunique matches\nby title & year" type="process"];
        map_title [label="Select\nunique matches\nby title" type="process"];
        map_popularity [label="Select\nmost popular candidates\nby title" type="process"];
    }}
    subgraph cluster_1 {{
        style=invis
        ignored [label="Ignored films", type="data"]
        extended_dataset [label="Extended Dataset\n\nid,title_imdb,\ntitle_tvtropes,\nyear,genres,\ntropes", type="data"]
    }}
    subgraph cluster_2 {{
        style=invis
        dataset1 [label="Tropes dataset\n\nfilms->tropes\n({n_films}->{n_tropes})" type="data"];
        dataset2 [label="IMDb Dataset\n\ntitle.basics.tsv\n(id,title,\nyear,genres)" type="database"];
        dataset3 [label="IMDb Dataset\n\ntitle.ratings.tsv\n(id,rating,votes)" type="database"];
    }}
    start->retrieve

    retrieve->dataset1[constraint=false];
    retrieve->dataset2[constraint=false];
    retrieve->dataset3[constraint=false];
    dataset1->dataset2[style=invis]
    dataset2->dataset3[style=invis]

    retrieve->titles;
    map_title_year->extended_dataset[constraint=false];
    map_title->extended_dataset[constraint=false];
    map_popularity->extended_dataset[constraint=false];
    map_popularity->ignored;
    titles->map_title_year->map_title->map_popularity;
}}'''

draw_graphviz(workflow, "mapper_workflow.pdf")
@

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/mapper_workflow.pdf}
\caption[Scraper]{Extending the dataset with genres and rating.\label{fig:mapper_workflow}}
\end{figure}

Items in IMDb that don't relate to films are excluded (tvEpisode, tvSeries, tvSpecial, tvShort, videoGame,
tvMiniSeries, titleType) because they are not in our TVTropes scraped dataset and they would only increase
ambiguity as more films might match the same name. % Maybe you're devoting too much time to
% this. I think this should better go to a separate technical report. In a
% journal paper you need to focus on what's innovative, not so much on the mechanics
% - JJ
As described in Figure~\ref{fig:mapper_workflow}, in order to be able to map the film names,
films names are normalized in both cases, TVTropes and IMDb, converting camel-case to Title case, removing
non-alphanumerical values and extra blanks, splitting name and year when required, and converting to lowercase.
Normalized names in TvTropes and IMDb are matched, ideally \{1->1\}.
In order to reduce ambiguity, if the year is present in TvTropes's identifier,
we reduce the search to the specific year in IMDb.
If there are more than one match for a tvTropes's identifier in IMDb,
we just choose the one with the highest number of votes. % Present this as a problem, then your solution - JJ
This heuristic relies in the fact that both data sources (tropes and votes)
are maintained by different communities of users, enthusiasts in both cases,
so if there is a film in TVTropes and there are many films with the same name in IMDb,
it will probably be the one with highest popularity, that is reflected in the number of votes.
We decided to include the original title so that non-English titles can be handled as well.


<<echo=False>>=
extended_dataframe = read_dataframe(FILM_EXTENDED_DATASET_TABLE_BZ2_FILE, USE_HDF)
trope_names = [key for key in extended_dataframe.keys() if key not in EVERYTHING_BUT_TROPES and '[GENRE]' not in key]
extended_dataframe['Number of tropes'] = sum(getattr(extended_dataframe,key) for key in trope_names)

coefficient_rating_votes, p_value_rating_votes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Votes'])
coefficient_rating_ntropes, p_value_rating_ntropes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Number of tropes'])
coefficient_votes_ntropes, p_value_votes_ntropes = stats.pearsonr(extended_dataframe['Votes'], extended_dataframe['Number of tropes'])
coefficient_year_ntropes, p_value_year_ntropes = stats.pearsonr(extended_dataframe['Year'], extended_dataframe['Number of tropes'])
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Number of tropes', y='Rating', color='darkgray', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Number of tropes', 'Rating', 'black')
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_rating_ntropes%> \\ p-value & <%=p_value_rating_ntropes%> \end{tabularx}}
        \label{fig:rating_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Votes', y='Number of tropes', color='darkgray', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Votes', 'Number of tropes', 'black')
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_votes_ntropes%> \\ p-value & <%=p_value_votes_ntropes%> \end{tabularx}}
        \label{fig:votes_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Year', y='Number of tropes', color='darkgray', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Year', 'Number of tropes', 'black')
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_year_ntropes%> \\ p-value & <%=p_value_year_ntropes%> \end{tabularx}}
        \label{fig:year_ntropes}
    \end{subfigure}
    \hfill
    \caption{Scatter plot to show the relation between: \textbf{(a)} Rating vs. \# of tropes
    \textbf{(b)} Votes vs. \# of tropes \textbf{(c)} Year vs. \# of tropes}
    \label{fig:extended_dataset_scatterplots}
\end{figure}

The resulting dataset includes the identifier in IMDb, in TVTropes, the name of the film, the year, the rating,
the number of votes, the tropes for the film (0 if not present, 1 if present) and the genre (0 if not present,
1 if present).

A brief analysis of the new extended dataset,
consisting in <%=extended_dataframe.shape[0]%> films linked to <%=len(trope_names)%> tropes,
shows that there are significant positive correlations between the number of tropes
and the rating, the popularity and the year, as shown in Figure \ref{fig:extended_dataset_scatterplots},
% DONE: Generación del modelo surrogado: Red Neuronal + descripción de parámetros estándar + decisiones + (en proceso de ver lo mínimo que hay que evaluar)

% These steps 1 and 2 are basically the creation of the training set for
% the surrogate model, together with some initial analysis which is not relevant
% to the paper at large. It would be probably much better to publish it separately
% as a technical report in ArXiV - JJ

\subsection{Step 3: Training the surrogate model}  \label{sec:methodology_surrogate_model}

<<echo=False>>=
input, output = get_experiment_execution_information(EVALUATOR_BUILDER_LOG_FILE)
layers = output[output['parameter']=='Layer sizes']['value'][0].split(', ')
@

Given our \textit{Extended Dataset}, % Better name pending, any ideas?
our goal is to build a trope-to-rating mapper
that can be used as a surrogate model by the Genetic Algorithm, in further steps.

The inputs are binary values for every trope and genre defined in the Extended Dataset
and the output is a continuous numeric value that represents the rating of the film, theorically from 0 to 10.
As the average number of tropes by film is 53, and the possible number of tropes is <%=n_tropes%>,
99\% of the cells in the matrix will have a value of 0, in other words, it will be very sparse,
with a small representation of the tropes in the catalogued films.
The deep learning technique most suitable in this context needs to expose feature-extraction capabilities
in order to deal with unknown and unbalanced relations between tropes to achieve a specific rating.
Although there are different candidates that could perform properly under these circumstances,
in our current research, we choose a Neural Network

The goal of this research is to evaluate a methodology, so
it is out of the scope to focus on the fine tuning of the surrogate model,
as far as it suits the needs in terms of quality of the estimations,
with reasonable performance, a number of false positives and
false negatives.

There are many decisions that can define the quality of the model;
some of them will be made based on the state of the art
and, for others, we will have to make hyper-parameters search.
In general, although there are many rules of the thumb to build acceptable neural networks,
results may differ drastically depending on the nature of the problem and
it is recommended to do a hyper-parameters evaluation. % cite required

We selected the Multi-layer perceptron (MLP), the most widespread neural network architecture,
because it has been proven to be able to approximate any function that we require,
the so calles universal approximation theorem.

In order to choose hyper-parameters that get along with the nature of our problem
we did a preliminary search with all the combinations in a domain of possible values
for the activation (ReLu or tanh),
the number of hidden layers (1 or 2),
the number of neurons in each layer (162 or 883/29),
the learning rate (contant or adaptative)
and the solver (Adam or SGD).
We applied 3-fold cross validation and got the average and the standard deviation.

<<echo=False>>=
iterations_evaluator = extract_iterations_from_log(log_file_name=EVALUATOR_BUILDER_LOG_FILE)
gridsearch_dataframe = extract_grid_parameters_from_log_and_results(log_file_name=EVALUATOR_HYPER_PARAMETERS_LOG_FILE)
@

\begin{figure}
    \centering
<%=print(get_table_for_dataframe(gridsearch_dataframe))%>
    \caption{Hyper-parameters evaluation using 3-fold cross validation, sorted by validation score}
    \label{fig:gridsearch_results}
\end{figure}

\begin{figure}
    \centering
<<width=".5\\linewidth",echo=False, results='hidden'>>=
plot = iterations_evaluator.plot(x='iteration', y=['loss','validation'], logy=True, color='black', linestyle='-')
plot.lines[1].set_linestyle('--')
leg = plot.legend()
plot.grid()
@
    \caption{Evolution of the loss and the validation score throughout the training iterations}
    \label{fig:evaluator_training}
\end{figure}

The results in figure~\ref{fig:gridsearch_results} show that
a MLP with the structure <%='/'.join(layers)%>, using ReLu activation,
constant learning rate and SGD solver provides the best validation score.
After training the MLP using the hyper-parameters, the Extended Dataset as input
and the rating as output, until it does not improve more than the tolerance for 10 consecutive runs,
the evaluation converges to a loss of <%=iterations_evaluator['loss'].iloc[-1]%>
and a validation score of <%=iterations_evaluator['validation'].iloc[-1]%>,
as we can see in Figure \ref{fig:evaluator_training}.

<<echo=False>>=
info_file = u'../datasets/evaluator_tests.json.bz2'
test = EvaluatorTests()
test.init_from_file(info_file)
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes = pd.DataFrame(test.test_results['errors_by_tropes'])
errors_by_tropes.plot.hexbin(x='Number of Tropes', y='Error', gridsize=25, cmap="Blues", bins="log", figsize=(5, 5.2))
plot.set_xlabel("Number of Tropes")
@
        \caption{}
        \label{fig:errors_by_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes_stats = pd.DataFrame(test.test_results['errors_by_tropes_stats'])
errors_by_tropes_stats.plot.scatter(x='Number of Tropes', y='Average', figsize=(5, 5.2))
@
        \caption{}
        \label{fig:average_errors}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes_stats = pd.DataFrame(test.test_results['errors_by_tropes_stats'])
errors_by_tropes_stats.plot.scatter(x='Number of Tropes', y='Standard Deviation', figsize=(5, 5.2))
@
        \caption{}
        \label{fig:average_std}
    \end{subfigure}
    \caption{\textbf{(a)} Hexagonal binning plot of the evaluation errors for all the films (color in logaritmic scale).
    \textbf{(b)} Average value of the error by number of tropes.
    \textbf{(c)} Standard deviation of the error by number of tropes}
    \label{fig:errors}
\end{figure}

% Análisis de error promedio por número de tropos
% Ejemplos de pelis conocidas con muchos y pocos tropos
% probar 1 género y ver qué sale

% DOING: Generación de conjuntos de tropos mediante GAs: Descripción del algoritmo + operadores + parámetros
\subsection{Film DNA builder}  \label{sec:methodology_ga}

% explicar el paso
At this step we already have a set of candidate tropes for the Film DNA and a model
that allows its evaluation, so our goal is to use a device that allows us to generate a Film DNA that optimizes the rating.

% Que es un GA y por qué es interesante
We chose to use a Genetic Algorithm because it is a mechanism that allows us to explore the domain of Film DNAs,
in other words, of combinations of tropes, getting high-quality solutions and dealing with global optimization problems.
% cite

% Trabajamos con tropos, que es un cromosoma
Our chromosome will be the DNA film, that is, a set of tropes.
In practice it will be encoded as an array of different indices on a dictionary of the total set of tropes available
without value repetitions, given that, \textit{a priori}, the order of the tropes is irrelevant
(according to their nature, they can refer to specific moments, but also to narrative structures and general settings)
and also, our rating evaluator does not consider weights or multiple occurrences of the trope in the movie
(both, training data and model do not consider multiple occurrendes of a trope in a single Film, in other words,
the trops appears or does not appear).

% La Mutación
The mutation operator changes a trope of the Film DNA by another that is randomly chosen
from the set of tropes ou of the Film DNA. By doing this, the Film DNA keeps complying with its restrictions,
mentioned above, and allows an exploration of new tropes.

% El cruzamiento
The crossover operator will make a superset of tropes from the parents' Film DNAs
and randomly selects two subsets for the offspring.
This way, the offspring will have Film DNAs whose tropes are exclusively from their parents, allowing
the exploitation of the data.


% La función fitness y el modelo surrogado
As we explained above, we have to rely on a surrogate model for the evaluation of Film DNAs, and our approach uses
a neural network trained with existing movies. The fitness of our GA will be the result of evaluating a DNA trope with
the neural network.

According to pour tests, the genetic algorithm has proven to converge towards optimal solutions efficiently,
given the simplicity of the operators based on set algebra, and fitness calculation, in this case, a trained neural network.

\section{Experimental setup} \label{sec:experimental_setup}

The evaluation of the methodology is performed unequivocally through its testing in different scenarios
and the analysis of results.
In this case, different applications of the methodology have been chosen,
which will help us answer the following questions:
Can we try to generate the optimal Trope DNA for a standard movie?
Could we fix the number of genres while maximizing the rating? % this question is not in the Introduction
% And it's not clear why this should be a problem (or why it's mentioned in the experimental setup) - JJ
Could we use this methodology to improve an existing Trope DNA?
% I don't think this should go in this paper either - JJ
How much do our results resemble existing films?


Our experiment will use a fixed size Film DNA of 30 tropes, which is the median of tropes per film
in the original dataset.

The most suitable parameters to solve a specific problem require an extensive calibration, which is outside
of the scope of this paper. However, to choose the set of parameters that we will use in all the experiments
in this paper, we have made a preliminary selection executing all possible combinations of Population (P) = \{50, 100, 200\},
Mutation probability (Mp) = \{\(2\div size_{dna}\), \(1\div size_{dna}\), \(0.5\div size_{dna}\)\}
and Crossover Probability (Cp) = \{0.25, 0.5, 0.75\}
and we have chosen the combination P = 200, Mp = \(1\div size_dna\) , Cp = 0.5, because it obtains better results on average.

In the experiments we have set the stop criterion to a minimum of 3000 evaluations and
until the best generation does not improve during 10000 evaluations.

% Say something about the results: we will use the best X out of every run, the best
% in the run with the best fitness... - JJ

% "A trope is not a full-fledged film, so we need to have an idea of what a movie
% with that set of tropes would look like. This is why ... " - something like this
% But also it would be interesting to show simply the score the surrogate assigns to some movies
% In fact, it would be better. Since the similarity is small, that means that
% comparing it with known films shows you what our generated movies _do not_ look
% like, not what they look like
% Anyway, it's important to motivate this and tell why we are doing it - JJ
As tools to interpret the results, we will use two metrics of similarity between finite sample sets.
The first one is a metric called \textit{Jaccard coefficient}
and is defined as the cardinality of the intersection of Film DNAs divided by the cardinality of the union of the Film DNAs.
The second one is called \textit{Overlap coefficient}
and is defined as the cardinality of the intersection of Film DNAs divided by the smallest length of the two Film DNAs.
Both metrics are interesting because we are dealing with incomplete data and we cannot assume that a a Film that has few tropes in its DNA
is actually extensively defined and described by the wisdom of crowds in our Extended Dataset as well as other popular films.
% TODO Citar las métricas
% More important, show references why this makes sense - JJ
\section{Results}

\subsection{Finding the best Trope DNA without genre limitations}

The repetition of the Genetic Algorithm 30 times has resulted in an average fitness of 9.783487634688873 (+- 0.4307202616394516).
The best solution has a rating of 10.313031174658876 and a Film DNA with the following tropes.
% Ratings go from 0 to 10, right? So it does not really make a lot of sense... - JJ


DNA = \{ActionHeroBabysitter, DeathByFlashback, DisneyVillainDeath, DuelToTheDeath,
EarlyBirdCameo, FightingFromTheInside, HandsOffParenting, Homage, ImNotAfraidOfYou,
JumpCut, MouthingTheProfanity, NoSympathy, OminousFog, OneHeadTaller, PoorMansSubstitute,
PragmaticAdaptation, RichIdiotWithNoDayJob, SomeoneToRememberHimBy, SpitefulSpit, TalkingHeads,
TitledAfterTheSong, WeaponOfXSlaying, [GENRE]Animation, [GENRE]Documentary, [GENRE]Drama,
[GENRE]History, [GENRE]Mystery, [GENRE]Romance, [GENRE]War, [GENRE]Western\}


<<echo=False>>=
checker = TropesSimilarityChecker()
checker.load_extended_dataset_json(FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE)
tropes_list = ['ActionHeroBabysitter', 'DeathByFlashback', 'DisneyVillainDeath', 'DuelToTheDeath',
               'EarlyBirdCameo', 'FightingFromTheInside', 'HandsOffParenting', 'Homage', 'ImNotAfraidOfYou',
               'JumpCut', 'MouthingTheProfanity', 'NoSympathy', 'OminousFog', 'OneHeadTaller',
               'PoorMansSubstitute', 'PragmaticAdaptation', 'RichIdiotWithNoDayJob', 'SomeoneToRememberHimBy',
               'SpitefulSpit', 'TalkingHeads', 'TitledAfterTheSong', 'WeaponOfXSlaying', '[GENRE]Animation',
               '[GENRE]Documentary', '[GENRE]Drama', '[GENRE]History', '[GENRE]Mystery', '[GENRE]Romance',
               '[GENRE]War', '[GENRE]Western']
top_overlap_exp1_1, top_jaccard_exp1_1 = checker.get_top_films_by_simmilarity(tropes_list, 5)
@

The syntethic Film DNA belongs to a multi-genre film (historical documentary romantic drama set during a war that includes
western and mistery settings, not comedy).
However, according to one of the tropes, even if it is historical/documentary,
it is completely an adaptation from the author with clear differences with the overall known story for pragmatic reasons (PragmaticAdaptation).
There are tropes that point to specific features of a hero:
The 'hero' is a rich character with free time (RichIdiotWithNoDayJob),
that has a position of responsibility for children (ActionHeroBabysitter),
that could be related to the existence of irresponsible parents (HandsOffParenting).
There are tropes that define the conflicts and how they will develop:
Someone resisting the controlling influence acting on them ('FightingFromTheInside'),
a villain is finally fought by the hero when he is 'not afraid of' anymore (ImNotAfraidOfYou),
there is a duel (DuelToTheDeath),
a special weapon is used (WeaponOfXSlaying),
the villain falls off (DisneyVillainDeath),
and the hero also dies.
There are tropes that points to a romantic relation between the hero and a girl,
that in some scenes is be clearly taller or shorter than him ('OneHeadTaller),
and that discovers, in the end, that is pregnant (SomeoneToRememberHimBy).
There are also tropes that define the setting, in this case, there is fog (OminousFog) as part of the mystery genre
and a clear 'Homage' to a classic or well known artwork in the same genres is present.
Regading the narrative perspective,
the story includes a flashback that points to the death of the main character/s (DeathByFlashback),
uses 'Jump Cuts' (JumpCut) and make a main character appear before his/her introduction (EarlyBirdCameo).
Some tropes also define very specific sequences; in some cases the film includes spits (SpitefulSpit)
and the characters swear, although it cannot be heard by the audience (MouthingTheProfanity).
There is also a scene where terrible things have happened to the main characters but no-one acts as it is really important (NoSympathy),
and there are scenes with no action, just long conversations where people do not move (TalkingHeads).
The meta-tropes present, on the other hand, include
an actor or actress is be relatively unknown but looks alike another well known one (PoorMansSubstitute)
and the film's name is a reference to an existing song.

% After the descripton above, you still have to give this an angle. For instance,
% can we find some similarity to existing films?
% There's another problem here: most very popular films have up to 800 tropes,
% our has only 30. So it's never going to be similar using these metrics to similar films.
% Again, this comparison opens many angles of attack and I don't think it really helps
% to convey the simple message that it's going to be very different from current films - JJ


% Also, this is another reason why we need tropes2vec for comparisons. It will
% make this arithmetic easier - JJ
The Synthetic Film DNA is hard % is hard?  - JJ
combination of genres, and this is why, in the next experiment, the number of genres is fixed.
% Really, I don't think we need this - JJ
Anyway, it is fun to analyze, % that need not be a plus. Precision and hard facts - JJ
as it reminds to films as 'Gone with the Wind', 'A Year of the Quiet Sun' or 'Man of the West'
in the sense that they combine multiple genres that are present in our Synthetic Film DNA. If we ignore the genres,
we can observe that it reminds to films like 'A Midwinter’s Tale', because of the reference in the Title or the dialogs without action,
'The Postman (Il postino)' because they both include the idea of 'gone, but not forgotten',
'Jane Eyre' or 'Tristan and Isolde' for the pragmatic adaptation of the author and some narrative details.

It is important to remark that, in all the cases, the Jaccard metrics indicate that the Film DNA is genuine,
and most of the films with high rating that resembles it are films with incomplete trope development.
In the best cases, there are 3 common tropes at maximum. % This is the only take
% away message and probably the only paragraph that should be left - JJ

These results are interesting and directly applicable to the generation of scripts.
However, we consider that our methodology leads to Film DNAs that promote the appearance of many genres at the same time
and that could be hard to implement or, in some cases, incompatible. The next experiment tries to fix this situation.

% Not worth the while to keep this subsection - JJ
\subsection{Finding the best Trope DNA with genre limitations}

This experiment sets the number of genres to the average, X, according to the extended dataset.
In order to implement it, % say precisely what the limitation is about. Number of genres?
% Why do you want to limit that? If it's because they're inconsistent, you should
% use other methods - JJ
 we changed the fitness function so
that it punishes solutions with a number of genres different to X, reducing the fitness by 5 points. This technique has been used
for ... in .. (citas). % Don't leave it this way, use comments for this - JJ

% Can't you use an automatic value for this?
The repetition of the Genetic Algorithm 30 times has resulted in an average fitness of
aaaa (+- bbbbb).  The best solution has a rating of ccccccc and a Film DNA with the following tropes.

% <TO ADD>
% Don't leave this text open, you can always forget


The 5 films that have the most tropes in common with this Film DNA are:


<TO ADD>


The minimum set of films that, together, maximize the number of common tropes, are.


<TO ADD>

Although the rating is lower than in the previous experiment, a Film with the provided Film DNA is easier to design as we
are not dealing with big genre incompatibilities.

% Maybe keep it, maybe not. If you want to prove the worth of the surrogate
% model, maybe, but I don't see what's the point of this - JJ
\subsection{Improving the Film DNA of a known film}

Could our methodology keep the essence of a film, that is translated to keeping most of the tropes,
while adding some new tropes in the Film DNA that optimize the note?

It is totally out of our intention to question a work of art like Star Wars, episode IV,
however, we feel a special predilection for this masterpiece of all time
and we consider that it is an appealing example for many of our readers,
that's why we are using this film as an example.


We will try to find Film DNAs that share at least 20 tropes with that film and let the GA find the rest, up to 30,
so that the new Film DNA of size 30 improves the rating of the original film.

In order to make this experiment we decide to implement a variant of the fitness function of the Genetic Algorithm
that punishes solutions that do not share at least 20 tropes with the film, reducing the fitness by 5 points. This technique has been used
for ... in .. (citas).

The repetition of the Genetic Algorithm 30 times has resulted in an average fitness of
aaaa (+- bbbbb).  The best solution has a rating of ccccccc and a Film DNA with the following tropes.


<TO ADD> (show original tropes in one color and the new in other)

The films that better matches the added tropes are:

This results are interesting

% It would be far more important to prove that the surrogate model works.
\section{Discussion} \label{sec:discussion}

\section{Conclusions} \label{sec:conclusions}
% TODO: Objetivo ha sido ...
% TODO: Para ello hemos realizado tal (metodología y experimentos) ...
% TODO: Los resultados nos dicen que ...
% TODO: Esto puede ser la base para una futura linea...

\section{Bibliography}
\bibliography{report}

\end{document}
