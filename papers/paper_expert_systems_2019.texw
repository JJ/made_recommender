<<echo=False>>=
import os
import pandas as pd
import scipy.stats  as stats
import matplotlib.pyplot as plt
import sys
sys.path.append(os.getcwd())

from utils import *

FILM_TROPES_JSON_BZ2_FILE = '../datasets/scraper/cache/20190501/films_tropes_20190501.json.bz2'
FILM_EXTENDED_DATASET_TABLE_BZ2_FILE = '../datasets/extended_dataset.csv.bz2'
FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE = '../datasets/extended_dataset.json.bz2'
USE_HDF = True
SCRAPER_LOG_FILE = '../logs/scrape_tvtropes_20190501_20190512_191015.log'
MAPPER_LOG_FILE = '../logs/map_films_20190526_164459.log'
EVALUATOR_BUILDER_LOG_FILE = '../logs/build_evaluator_20190624_223230.log'
TOP_VALUES = 14
EVERYTHING_BUT_TROPES = ['Id','NameTvTropes', 'NameIMDB', 'Rating', 'Votes', 'Year']
EVALUATOR_HYPER_PARAMETERS_LOG_FILE = '../logs/build_evaluator_hyperparameters_20190622_203043.log'


films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)


n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())
@

\documentclass[APA,LATO1COL]{WileyNJD-v2}
\usepackage[utf8]{inputenc}
\usepackage{minted}
%\usepackage{moreverb}

% -- begin: Added by Rubén
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{tabularx}
\usepackage{ragged2e}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
% -- end: Added by Rubén


\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\articletype{Article Type}%

\received{<day> <Month>, <year>}
\revised{<day> <Month>, <year>}
\accepted{<day> <Month>, <year>}

%\raggedbottom

\begin{document}

%\title{Chasing compelling stories using deep learning and tropes}
\title{StarTroper, a film trope rating optimizer using Deep Learning and Evolutionary Algorithms}

\author[1]{Rubén Héctor García-Ortega}

\author[2]{Pablo García-Sánchez}

\author[3]{Juan Julián Merelo-Guervós}



\address[1]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[2]{\orgdiv{Org Division}, \orgname{Org name}, \orgaddress{\state{State name}, \country{Country name}}}

\address[3]{\orgdiv{Department of Computer Architecture and Technology}, \orgname{University of Granada}, \orgaddress{\country{Spain}}}

\corres{*Rubén Héctor García Ortega, Badger Maps. \email{raiben@gmail.com}}

\presentaddress{Present address}

\abstract[Abstract]{
Designing a story is widely considered a crafty yet critical task that requires deep specific human knowledge
in order to reach a minimum quality and originality. This includes designing at a high
level different elements of the film; these high-level elements are called tropes when they become patterns.
The present paper proposes and evaluates a methodology to automatically synthesise sets of tropes
in a way that they maximize the potential rating of a film that conforms to them.
We use deep learning to create a surrogate model mapping film ratings from tropes,
trained with the data extracted and processed from huge film databases in Internet,
and then we use a Genetic Algorithm that uses that surrogate model as evaluator
to optimize the combination of tropes in a film.
In order to evaluate the methodology, we analyse the nature of the tropes and their distributions in existing films,
the performance of the models and the quality of the sets of tropes synthesised.
The results of this proof of concept show that the methodology works and is able to
build sets of tropes that maximize the rating and that these sets are genuine.
The work has revealed that the methodology and tools developed
are directly suitable for assisting in the plots generation as an authoring tool
and, ultimately, for supporting the automatic generation of stories, for example, in massively populated videogames.}

\keywords{Content Generation; Tropes; Computational Narrative; Deep Learning; Genetic Algorithms}

\jnlcitation{\cname{%
\author{R.H. Garc\'ia-Ortega},
\author{J.J. Merelo},
\author{P. Garc\'ia-S\'anchez}} (\cyear{2019}),
\ctitle{Creating compelling stories using computational intelligence and tropes}, \cjournal{}, \cvol{}.}

\maketitle


\section{Introduction}\label{sec1}

% Define context first: what's a trope, what is the overall motivation - JJ
The goal of the paper is to demonstrate how computational intelligence can be used to generate and improve
sets of tropes that maximize the potential rating of the films that conforms them,
in the context of authoring tools and Content Generation.

% Maybe this paragraph should go first - JJ
Crafting film scripts is quite challenging because of the plot complexities and the
\textit{multiplicative production function of entertainment} ~\citep{Hennig-Thurau2019},
that promulgate that the elements involved in the development of a media product need to work together
and a single failing one may provoke a disaster in cascade.
In order to consider this multiplicative function in the current research, we need to describe the elements of the film
and how well they combine; our candidates for both mechanisms are the
tropes that have been discovered in the films, and the massive human-evaluated ratings, respectively.
We can roughly characterize a film
by using the set of tropes that we can find in it.
A trope is as a recurring narrative device or pattern, according to the definition by \cite{baldick2015oxford};
it could be a technique, a motif, an archetype or a \textit{clich\'e},
used by the script writers, producers and directors
to achieve specific effects that might vary from interest-increasing
to surprising through recall familiarity or entertaining,
in their creative works, such as books, films, comics or videogames.
Some tropes are broadly adopted and academically studied such as
the \textit{Three-act Structure} formulated by \cite{field1982screenplay},
the \textit{Hero's Journey} studied by \cite{vogler2007writer},
the \textit{McGuffin} popularized by Hitchcock, according to \cite{truffaut1985hitchcock}, and
the \textit{Chekhov's Gun} formulated by the Russian writer with the eponymous
name, according to \cite{bitsilli1983chekhov};
however, there are thousands of not-so-widely used tropes as well, discovered and
catalogued everyday by professionals and enthusiastic of the storytelling;
their study is organic, dynamic and extensive, according to~\cite{garcia2018overview}.
In general, we might say that the set of tropes defines the overall narrative
architecture over which the narrative layer is eventually set. Tropes do not
define plot univocally, but constrain it in a number of ways.

Along this paper, we use the analogy of the \textit{Film DNA}
and define it as the set of tropes that are found in a film
and define things such as its structure, characters, events, mood, settings and narration.
As the tropes are \textit{living concepts},
which grow as they are discovered as common
patterns in other stories, the \textit{Film DNA} is, by definition, incomplete and evolving,
yet it is still interesting to define stories, categorize them
and model them from a mathematical perspective.
The challenge of our research is to build original synthetic Film DNAs
based on a huge corpus of film-tropes, and through computational intelligence,
in a way that they have an intrinsic potential quality when reflected together in a film.

At the same time, we need to be able to associate a measure of quality to the \textit{Film DNA}
and it needs to summarize many factors, at last, perceived and evaluated by humans.
Luckily we have access to databases with films' information
that includes the genres of the films and their human evaluated ratings,
provided by the community of fans.
If we are able to construct a knowledge base of films, including the Film DNAs that we mentioned previously, the genres and the rating,
in a huge \textit{extended dataset}, we would be able to process them in order to make suggestions of tropes
that optimize the predicted rating;
however, even though intuitively
the \textit{Film DNA} is a profound way to describe a story
from many different perspectives, following the analogy of the DNA,
there are epigenetic factors that could deeply affect the performance of the story as well.
This method does not guarantee the quality of a film,
as a film that develops from a \textit{Film DNA} may implement them in infinite ways with very different
results in terms of quality;
however, it can be used as an indicator of the \textit{potential of the story} or the
most probable implementation based on the universe of currently analyzed films.

Our approach extracts <%=n_films%> \textit{Film DNAs} that contain in sum <%=n_tropes%> different tropes
from external Data Sources
and maps it to a database of film ratings and genres, dealing with disambiguation heuristics
in order to build what we have called the \textit{extended dataset}.
However, submitting a set of tropes to the box office is impossible, which is
why we use deep learning % TODO cite
to create a surrogate model that is able
to infer the rating from any combination of tropes.
We perform different analysis in order to determine the quality
of the predictions and the parameters that could affect them.
Later on, a Genetic Algorithm and their operators are defined in a way
that the trope combinations, formerly Film DNAs, are evolved relying in the surrogate model to
maximize the rating.

The remaining of this work is organized as follows:
in the Section~\ref{sec:state_of_the_art} we explore the current state of the art
in plot generation based on tropes, in the Section~\ref{sec:methodology} we deepen the methodology presented above,
in the Section~\ref{sec:experimental_setup} we describe the experiments carried out to evaluate the methodology
and discuss the results,
and in the Section~\ref{sec:conclusions} we summarize the outcomes and future work.

\section{State of the art} \label{sec:state_of_the_art}



% Formalismos narrativos y gramáticas

Some of the narrative formalisms that allow film-makers, videogame developers and researchers in computational narrative
to generate stories in a procedural manner have been proposed almost a century ago;
for example, the one suggested by \cite{propp2010morphology}, first edited in 1928 and still in use today,
is based on seven different roles, every one with a list of actions that can take over the course of a story,
in a fixed sequence of 31 functions; however, it is simply limited to expressiveness,
as it is not possible to create new functions.
% limited to expressiveness? Do you mean its expressiveness is limited? - JJ
% revisits Propp’s morphology to build a system that generates instances of Russian folk tale (http://drops.dagstuhl.de/opus/volltexte/2013/4156/)
% y otro que tb usa propp grammar https://ieeexplore.ieee.org/abstract/document/6505320
% y otro https://ieeexplore.ieee.org/abstract/document/6138227
% tb hay un pollo que usa gramáticas, pero es más que interesante https://ieeexplore.ieee.org/abstract/document/5585934

%... Como ceñirnos a gramáticas de Propp es demasiado limitante,
Other authors propose the use of agents, each with actions and obligations,
that allow to guide them through the arc of a story.
This has advantages, as each agent can be programmed to have different behaviours based on psychological models,
according to \cite{Thompson18NarrativeEvents}. % but... - JJ
% Also: cite our MADE work - JJ

%Aunque la aproximación basada en agentes es muy interesante, consideramos que la idea de coger

%Muchos artículos se centran en el uso de de Propp's grammars, y está bien, para nosotros lo interesante
%es que basarse en Propp grammars es utilizar patrones, y lo que buscamos precisamente. Nuestra filosofía en esta investigación
%no será bajar a la granularidad de la narración, sino a una generalidad de patrones que permitan definir un guión a diferentes niveles
%mediante patrones, como el de Propp.
% But you have to mention it - JJ


Some authors have proposed the use of tropes, defined in the introduction,
as a way of structuring a story in a consistent and reusable way.
In the work of \cite{Thompson18NarrativeEvents}, a system of agents relies on tropes to obtain a consistent narrative,
to describe the social norms that model the world in which they live.
The authors, as in this work, use tropes available on TV Tropes as a base
and translate them into logic statements that express duty or obligation,
using TropICAL language, which are the input for a logic programming solver;
however, they do not use all the tropes,
but a small set chosen by hand, whicn means that the range of resulting stories
is going to be limited.

% Otros autores tb han usado tropes, como por ejemplo Hablar de GHOST: a GHOst STory-writer

% sin embargo, nuestra propuesta implica evitar la selección manual de los tropes y confiar en Hablar de %smarter than even individual experts, according to AAA.%James Surowiecki in his 2004 book, The Wisdom of Crowds

%El tema es que es muy dificil evaluar una historia, mucho más dificil es evaluar un conjunto de tropes que caracterizan una historia

Nevertheless, it is very complicated to evaluate the content generated by an automatic generator,
not only because of its non-deterministic behaviour that makes it difficult to predict its outputs,
but also because of the subjective, diverse and stochastic nature of the audience,
as stated by \cite{TogeliusCap4Evaluating}.

To evaluate a generator one can use directly the opinion of the designer,
or indirectly from the audience, for example, using surveys. % por ejemplo https://ieeexplore.ieee.org/abstract/document/5585934
But another possibility is the use of Artificial Intelligence,
by simulating and estimating the quality of the content via some metrics.
Moreover, through the use of user-generated data, it is possible to obtain a large corpus
of examples to be used in computational narrative, as in th work of \cite{Guzdial15Crowdsourcing}.
In fact, it is possible to extract information about review
sites, according to \cite{BoPang08OpinionMining}, such as MetaCritic or IMDb,
to be the input of a model like the one we propose in this article.

% buscar si hay referencias sobre que el uso de tropes ya implica interés

%%HABLAR DE LOS MODELOS SURROGADOS AQUI

% alguien los usa?
%alguien los usa para medir la calidad?
%alguien usa neural networks?

% alguien usa GAs para modelar plots o plot devices?

% regular grammar is developed to model various causal relationships inside a given story world. The grammar is then evolved using evolutionary computation techniques to generate novel story plots, i.e. story-based scenarios. (https://ieeexplore.ieee.org/abstract/document/5585934)
% Otra propone tree adjoining grammar (TAG) to capture semantics of a story with long-distance causal dependency --- using grammar guided genetic programming (GGGP) https://link.springer.com/chapter/10.1007/978-3-642-17298-4_14


Our previous work is based on some of the ideas mentioned above.
In the work by \cite{GarciaOrtega15MADE}, we proposed the MADE framework:
a parametrizable multi-agent system that allowed
the generation of backstories in massive environments.
A genetic algorithm was used to optimize the parameters of the system, for instance the simulation time,
the size of the world and the parameters of the behaviour of the agents,
with respect to the appearance of different archetypes, such as the {\em Hero} or the {\em Villain}.
These archetypes are defined by the possible actions that an agent may perform:
for example, the archetype Villain appears when an agent fights against another for food;
however, this form of evaluation was difficult to justify in order to measure the quality of the generated stories,
since it was based on an objective decision:
just the number of different character archetypes that emerge during the run of the world.
Also, the list of possible actions for the agents was very limited.
That is the reason that, later on, we proposed in the work by \cite{GarciaOrtega16MADE2},
a more advanced model, with more complex agents and the possibility of extracting knowledge from a logical reasoner.
On this occasion we used as quality metric the appearance of the archetype of the Monomyth, also known as The Hero's Journey,
and the different archetypes that compose it.
In order to do this, logical reasoning was applied based on the predicates produced by the different events
that emerged in the system; however, as in previous work, the mere appearance of the monomyth does not fully serve
as a measure the interest of the stories generated by the system.

That is why in this work we propose the combination of the previous ideas:
% usar tropes como medida de elementos interesantes
% confiar en el wisdom of the crowd de tvtropes
% confiar en la nota de IMDb
% confiar en modelos surrogados para la evaluación, específicamente, redes neuronales
% confiar en un GA como modelo para la generación de tropes
to use user-generated data that describes the film and film scores to build a surrogate computational model
and use it to evaluate the subjective quality of a story described by a set of tropes.

% TODO: conclusión: lo que vamos a hacer es mejor

\section{Methodology} \label{sec:methodology}


Our methodology is divided in four main steps, explained below and described
in the Figure~\ref{fig:main_workflow_extended}:

<<echo=False>>=
workflow = f'''
digraph {{
    splines=polyline
    rankdir=LR
    ranksep=0.25;
    margin=0;
    nodesep=0.3;
    graph [ resolution=128, fontsize=30];

    node [margin=0 fontcolor=black fontsize=10 width=1];
    tvtropes[label="TV Tropes\nwebsite\n\ntvtropes.org\n " type="database"];
    scrape_tropes[label="Step 1:\nScrape tropes\n\nPython+\nrequests+\nlxml+\nbz2 (blocksize 900k)+\ndisk cache\n~11.900 pages" type="process"];
    dataset[label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})\n " type="data"];
    imdb[label="IMDB\ndatasets:\nimdb.com/\ninterfaces/" type="database"];
    map_rating[label="Step 2:\nDisambiguate\nfilms\n\nPython+\nHeuristics+\nbz2" type="process"];
    extended_dataset[label="Extended\nDataset\n\nFilm DNA+genres->\nrating" type="data"];
    build_evaluator[label="Step 3:\nBuild\nSurrogate\nModel\n\npandas+\nsklearn (MLPRegressor)\n " type="process"];
    evaluator[label="Surrogate model\n\nFilm DNA+genres->\nExpected\nRating\n\nMulti-layer\nPerceptron" type="tool"];
    user[label="User's\nconstraints\nfor the\nSynthetic\nFilm DNA" type="data"];
    dna_builder[label="Step 4:\nGenetic Algorithm\n\ninspyred+\ncachetools" type="process"];
    trope_sequence[label="Optimal\nSynthetic\nFilm DNA" type="data"];

    tvtropes -> scrape_tropes[minlen=0];
    scrape_tropes -> dataset[minlen=1];
    dataset -> map_rating;
    imdb -> map_rating[minlen=0];
    map_rating -> extended_dataset;
    extended_dataset -> build_evaluator;
    build_evaluator -> evaluator;
    evaluator -> dna_builder;
    user -> dna_builder[minlen=0];
    dna_builder -> trope_sequence;
}}'''

draw_graphviz(workflow, "main_workflow_extended.pdf")
@

\begin{itemize}
    \item[Step 1] Extract/scrape the tropes for every film
    and codify them as \textit{Film DNAs}.
    As we will explain, our dataset will have limitations derived
    from the fact that is fed from
    the community, finding that popular films are broadly described in terms of tropes
    and unpopular films poorly described or directly missing.
    We will analyze this variability and how it could affect the performance of the prediction model.
    \item[Step 2] Extract ratings and genres from an external Film Database finding the unequivocal film names
    and cull the original TV Tropes dataset.
    This \textit{extended dataset}  will show limitations as well
    based on the original one
    and the automatic matching based on different heuristics.
    As we will see, a trope that is widely used does not need to be linked to good ratings,
    tropes that are present in bad films can become good in different combinations and vice-versa.
    \item[Step 3] Build and train a surrogate model to predict the rating from a \textit{Film DNA}.
    We will follow different \textit{rules of thumb}
    to achieve a moderately good solution that serves our purposes.
    A multi-layer preceptron will handle the unknown relations between tropes and their combinations.
    \item[Step 4] Optimize the Film DNA with respect to ratings by
    building a Genetic Algorithm with specific operators
    that relies in the surrogate model previously built.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/main_workflow_extended.pdf}
\caption[Methodology]{Methodology to generate constrained optimal Film DNAs using Genetic Algorithms with Neural
Networks as surrogate models, fed from TV Tropes and IMDb.\label{fig:main_workflow_extended}}
\end{figure}


% TODO: Extracción de tropes: Explicar brevemente el tema de TV Tropes, DBTropes y nuetsro scraper + explicar los datos (Gráficas y tops)

\subsection{Step 1: Extraction of tropes} \label{sec:methodology_scraper}
% Don't need to devote much time to this except if it's extraordinarily novel
% Refer to former report (PicTropes) and the upcoming one - JJ


<<echo=False>>=
tropes_summary_dictionary = {}
for key in films_dictionary:
    tropes_summary_dictionary[key] = {'tropes':len(films_dictionary[key])}

tropes_summary_dataframe = pd.DataFrame(tropes_summary_dictionary).transpose()

films_summary_dictionary = {}
for key in tropes_dictionary:
    films_summary_dictionary[key] = {'films':len(tropes_dictionary[key])}

films_summary_dataframe = pd.DataFrame(films_summary_dictionary).transpose()
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = tropes_summary_dataframe.plot.hist(log=True, color='green', figsize=(5, 5.2), zorder=2, rwidth=0.5)
plot.set_xlabel("Films by number of tropes")
@
        \caption{}
        \label{fig:histogram_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.sort_values('tropes',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_tropes_films}
    \end{subfigure}

    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = films_summary_dataframe.plot.hist(log=True, color='#1E77B4', figsize=(5, 5.2), zorder=2, rwidth=0.5)
plot.set_xlabel("Tropes by number of films")
@
        \caption{}
        \label{fig:histogram_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.sort_values('films',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_films_tropes}
    \end{subfigure}
    \caption{
    \textbf{(a)} Descriptive analysis of the Tropes by appearance in films.
    \textbf{(b)} Histogram of number of tropes by film.
    \textbf{(c)} Top films by number of tropes.
    \textbf{(d)} Descriptive analysis of the tropes by number of films in which they appear.
    \textbf{(e)} Histogram of number of films by tropes. Please note the logarithmic $y$ axis.
    \textbf{(f)} Top tropes by number of films.}
    \label{fig:films_analysis}
\end{figure}

We are going to use tropes as described in a live wiki called \cite{tvtropes_2}, that is
collecting thousand of descriptions and examples of tropes from 2014 until now.
As the data is fed by a community
of users, we could find the bias that popular films are better described and analysed in terms of the tropes
than older or independent films, and that popular tropes are more recognised than very specific ones. % REFERENCE NEEDED - JJ
Which means that, during the automatic generation of Film DNAs, tropes cound be under/overrepresented,
and that positive and negative estimation errors are possible.
The semantic network of knowledge behind TV Tropes is huge and complex; it massively links hierarchies of tropes
to their usage in creations for digital entertainment. The data, however, is only available through its web interface,
which is why, in order to make it usable by the scientific community, \cite{maltekiesel_2} extracted all
their data to a database so-called \textit{DBTropes.org}.
As the base of the research on automatic trope generation, we begun with a dataset based in the
latest version of DBTropes, called PicTropes~\citep{garcia2018overview} that included 5,925 films with 18,270 tropes.
However, the last version of DBTropes is from 2016, and the community of users of TV Tropes has tripled the size
of the database since then; in other words, we are not using it because it is outdated.
If we work with the latest data from TV Tropes our machine learning algorithms
would benefit from having much entries and hence, provide better results. That is why our first step is to
extract the data directly from TV Tropes while making it available to the public % TODO cite
and the researchers,
in the context of the Open Science.

Our scraper, which is also released as free software in the Python ecosystem under
the {\tt tropescraper} name, and is also available from GitHub
(\url{https://github.com/raiben/tropescraper}), %cite scraper
extracts all the categories from the main
categories page and, for every one of them, it extracts all the film identifiers assigned to it.
Finally, for every film page, it extracts all the trope identifiers, building
a dictionary of films and tropes.
Trope identifiers are written in \textit{CamelCase} format and may include the year to avoid ambiguity.
Some technical details are listed in Figure~\ref{fig:main_workflow_extended}.

The resulting dataset includes <%=n_films%> \textit{Film DNAs} and <%=n_tropes%> tropes.
In both cases, the number of tropes by film and film by tropes follow long tail distributions, where
a large number of occurrences are far from the "head" or central part of the distribution,
as shown in Figure~\ref{fig:films_analysis}.
Most of the films have 30 tropes, the median, % this is not true.
% 50% of films have 30 or less; 60% 40 or less, so you can only say that 60%
% films have 40 or less tropes - JJ
but there are films with more than 800 tropes.
On the other hand, most tropes appear in 6 films, but there are tropes with more than 3000 occurrences in films.
% Conclude with something that relates to the paper objectives. "These figures
% will have to be taken into account when... "

It is part of the current research to analyze the expected effect of this distribution
in the results of applying our methodology.
The first conclusion is that we have many more samples with a small number of tropes than with many;
however, at this step we do not have enough information to elucidate if this situation is explained
by the fact that it is user-generated data and the popularity
defines how well described are the films in terms of tropes.
% "But we can assume that, in general..." - JJ
Furthermore, we cannot make out yet a relationship between the number of tropes
in a film and its rating. % "But the figure xxx says..." - JJ
However, as the next section complements the tropes with additional information,
such as the rating, the genres or the number of votes,
we will be in a position where we can find correlations
that help us explain the possible results of the experiments in a better way.

\subsection{Step 2: Disambiguation of films to get the rating} \label{sec:methodology_mapper}

TV Tropes is a huge yet very specific database of tropes but it does not include a rating
or links to an external database that we could use as a rating source;
on the other hand, IMDb offers their database for non-commercial use and they provide
datasets with lots of interesting features,
including the rating and the number of votes.
Our research just needs a way identify a movie in TV Tropes with another in IMDb.

IMDb Datasets are a compendium of information that IMDb offers for personal and
non-commercial use. Both conditions of use and dataset descriptions are explained in
https://www.imdb.com/interfaces/. % Best as a reference. - JJ

Our current research will make use of these datasets to extend the film information from TV Tropes, in particular,
{\tt title.basics.tsv}, % filename is unneeded. Use dataset name - JJ
 which contains metadata from the films such as the title, the year, the genres
and the duration, and {\tt title.ratings.tsv}, which contains the rating and the number of votes.

Items in IMDb that don't relate to films are excluded (tvEpisode, tvSeries, tvSpecial, tvShort, videoGame,
tvMiniSeries, titleType) because they are not in our TV Tropes scraped dataset and they would only increase
ambiguity as more films might match the same name.
In order to be able to map the film names,
films names are normalized in both cases, TV Tropes and IMDb, converting \textit{CamelCase} format to \textit{Title case}, removing
non-alphanumerical values and extra blanks, splitting name and year when required, and converting to lowercase,
considering the original title and the English title.
Normalized names in TV Tropes and IMDb are matched, ideally \{1->1\}, but
in practice, especially when the year is not declared, we tend to find a big list of candidates
for every single film in TV Tropes.
In order to reduce ambiguity, if the year is present in TV Tropes's identifier,
we reduce the search to the specific year in IMDb, and, in any case,
we select the candidate with the highest number of votes.

This heuristic relies in the fact that both data sources (tropes and votes)
are maintained by different communities of users, enthusiasts in both cases,
so if there is a film in TV Tropes and there are many films with the same name in IMDb,
it will probably be the one with highest popularity, that is reflected in the number of votes.


<<echo=False>>=
extended_dataframe = read_dataframe(FILM_EXTENDED_DATASET_TABLE_BZ2_FILE, USE_HDF)
trope_names = [key for key in extended_dataframe.keys() if key not in EVERYTHING_BUT_TROPES and '[GENRE]' not in key]
extended_dataframe['Number of tropes'] = sum(getattr(extended_dataframe,key) for key in trope_names)

coefficient_rating_votes, p_value_rating_votes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Votes'])
coefficient_rating_ntropes, p_value_rating_ntropes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Number of tropes'])
coefficient_votes_ntropes, p_value_votes_ntropes = stats.pearsonr(extended_dataframe['Votes'], extended_dataframe['Number of tropes'])
coefficient_year_ntropes, p_value_year_ntropes = stats.pearsonr(extended_dataframe['Year'], extended_dataframe['Number of tropes'])
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Number of tropes', y='Rating', color='#1E77B4', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Number of tropes', 'Rating', 'black')
#plt.grid(b=True, which='major', color='#666666', linestyle='-')
#plt.minorticks_on()
#plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_rating_ntropes%> \\ p-value & <%=p_value_rating_ntropes%> \end{tabularx}}
        \label{fig:rating_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Votes', y='Number of tropes', color='#1E77B4', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Votes', 'Number of tropes', 'black')
#plt.grid(b=True, which='major', color='#666666', linestyle='-')
#plt.minorticks_on()
#plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_votes_ntropes%> \\ p-value & <%=p_value_votes_ntropes%> \end{tabularx}}
        \label{fig:votes_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Year', y='Number of tropes', color='#1E77B4', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Year', 'Number of tropes', 'black')
#plt.grid(b=True, which='major', color='#666666', linestyle='-')
#plt.minorticks_on()
#plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_year_ntropes%> \\ p-value & <%=p_value_year_ntropes%> \end{tabularx}}
        \label{fig:year_ntropes}
    \end{subfigure}
    \hfill
    \caption{Scatter plot (with fitting) to show the relation between: \textbf{(a)} Rating vs. \# of tropes
    \textbf{(b)} Votes vs. \# of tropes \textbf{(c)} Year vs. \# of tropes}
    \label{fig:extended_dataset_scatterplots}
\end{figure}

% This paragraph includes implementation details and is not needed - JJ
%The resulting dataset includes the identifier in IMDb, in TV Tropes, the name of the film, the year, the rating,
%the number of votes, the tropes for the film (0 if not present, 1 if present) and the genre (0 if not present,
%1 if present).

% You need to explain why we need this figure, and what it's telling us.
% You can't just drop figures _and then_ explain them. There's a narrative to
% a paper, and you need to justify the presence of the figure before you include
% it. "First, we need to explore if there are significant biases in the original
% set, since it's generated by the user... " - JJ
Figure \ref{fig:extended_dataset_scatterplots} shows the relations between the number of tropes and
the rating, the number of votes and the year in the new \textit{extended dataset},
consisting in <%=extended_dataframe.shape[0]%> films linked to <%=len(trope_names)%> tropes.
%
The Figure shows that there is a significant positive correlation between the number of tropes and the rating, in other words,
higher number of tropes are related to higher rating, and vice-versa.
It is important to remark that the films up to $\sim53$ tropes (the average) or less, show the widest range,
$\{\sim 1.3-\sim9.5\}$, for the rating,
whereas films with more than 600 tropes have a rating in the range $\{6-\sim9.5\}$.
If we want to test the methodology, choosing a fixed number of tropes lower or equal to $53,$ for example, the median (30),
would allow us to improve the solution in a wider range. At the same time, a number of tropes in this range
will be prone to have a rating in a range $\{6-6.5\}$, according to the regression function.

There seems to be also a significant positive correlation between the number of tropes and the votes (popularity) as well.
That explains the long queue distributions in the previous step, Figure~\ref{fig:films_analysis}, in other words,
the more popular the film is, the better it is described by the community and the more tropes are found.
% Say something about what this implies for your paper - JJ

Finally, there is a significant positive correlation between the number of tropes and the year,
according to the scatter plot, the year of the film limitates the maximum number of tropes that describe the films
in our dataset.

The three outcomes will help us unravel the limitations of a surrogate model fed from the \textit{extended dataset}
and used to predict the rating from a set of tropes. % But you need to conclude
% something, about the number of tropes and the type of movies it will generate - JJ

\subsection{Step 3: The surrogate model}  \label{sec:methodology_surrogate_model}

<<echo=False>>=
input, output = get_experiment_execution_information(EVALUATOR_BUILDER_LOG_FILE)
layers = output[output['parameter']=='Layer sizes']['value'][0].split(', ')
@

Given our \textit{extended dataset}, % Better name pending, any ideas?
our goal is to build a trope-to-rating approximator
that can then be used as a surrogate model by the Genetic Algorithm.

% Say how representation of tropes is important. Never present anything as the
% only option, but as a result of a thought process or comparison that leaves
% room for improvement in subsequent iterations - JJ
The inputs to the model will be binary values for every trope and genre defined in the \textit{extended dataset}
and the output is a continuous numeric value that represents the rating of the film, theorically from 0 to 10.
As the average number of tropes by film is 53, and the possible number of tropes is <%=n_tropes%>,
99\% of the cells in the matrix will have a value of 0, in other words, it will be very sparse,
with a small representation of the tropes in the catalogued films.
The deep learning technique most suitable in this context needs to expose feature-extraction capabilities
in order to deal with unknown and unbalanced relations between tropes to achieve a specific rating.
Although there are different candidates that could perform properly under these circumstances,
in our current research, we choose a Neural Network. % TODO cite

The goal of this research is to evaluate a methodology, so
it is out of the scope to focus on the fine tuning of the surrogate model,
as far as it suits the needs in terms of quality of the estimations,
with reasonable performance and a low error rate.

There are many decisions that can define the quality of the model;
some of them will be made based on the state of the art
and, for others, we will have to make hyper-parameters search.
In general, although there are many rules of the thumb to build acceptable neural networks,
results may differ drastically depending on the nature of the problem and
it is recommended to do a hyper-parameters evaluation. % TODO cite

We selected the Multi-layer perceptron (MLP), the most widespread neural network architecture,
because it has been proven to be able to approximate any function that we require,
the so called \textit{Universal Approximation Theorem}. % TODO cite

In order to choose hyper-parameters that get along with the nature of our problem
we did a preliminary search with all the combinations in a domain of possible values
for the activation (ReLu or tanh),
the number of hidden layers (1 or 2),
the number of neurons in each layer (162 or 883/29),
the learning rate (contant or adaptative)
and the solver (Adam or SGD).
We applied 3-fold cross validation and got the average and the standard deviation.
%
<<echo=False>>=
iterations_evaluator = extract_iterations_from_log(log_file_name=EVALUATOR_BUILDER_LOG_FILE)
gridsearch_dataframe = extract_grid_parameters_from_log_and_results(log_file_name=EVALUATOR_HYPER_PARAMETERS_LOG_FILE)
@
\begin{table}
    \centering
<%=print(get_table_for_dataframe(gridsearch_dataframe))%>
    \caption{Hyper-parameters evaluation using 3-fold cross validation, sorted by validation score}
    \label{fig:gridsearch_results}
\end{table}
%
The results in Table~\ref{fig:gridsearch_results} show that
a MLP with the structure <%='/'.join(layers)%>, using ReLu activation,
constant learning rate and SGD solver provides the best validation score.
After training the MLP using the selected hyper-parameters, the \textit{extended dataset} as input
and the rating as output, until it does not improve more than the tolerance for 10 consecutive runs,
the evaluation converges to a loss of <%=human_readable(iterations_evaluator['loss'].iloc[-1])%>
and a validation score of <%=human_readable(iterations_evaluator['validation'].iloc[-1])%>.



<<echo=False>>=
info_file = u'../datasets/evaluator_tests.json.bz2'
test = EvaluatorTests()
test.init_from_file(info_file)
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes = pd.DataFrame(test.test_results['errors_by_tropes'])
errors_by_tropes.plot.hexbin(x='Number of Tropes', y='Error', gridsize=25, cmap="Blues", bins="log", figsize=(5, 5.2))
plot.set_xlabel("Number of Tropes")
@
        \caption{}
        \label{fig:errors_by_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes_stats = pd.DataFrame(test.test_results['errors_by_tropes_stats'])
errors_by_tropes_stats.plot.scatter(x='Number of Tropes', y='Average', figsize=(5, 5.2))
@
        \caption{}
        \label{fig:average_errors}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes_stats = pd.DataFrame(test.test_results['errors_by_tropes_stats'])
errors_by_tropes_stats.plot.scatter(x='Number of Tropes', y='Standard Deviation', figsize=(5, 5.2))
@
        \caption{}
        \label{fig:average_std}
    \end{subfigure}
    \caption{\textbf{(a)} Hexagonal binning plot of the evaluation errors for all the films (color in logaritmic scale).
    \textbf{(b)} Average absolute value of the error by number of tropes.
    \textbf{(c)} Standard deviation of the absolute error value by number of tropes}
    \label{fig:errors}
    % I have tried to label the y axis better above, but failed - JJ
\end{figure}

The multi-layer perceptron will be used as a surrogate model in a specific experiment in further sections,
so, in order to anticipate the results, we need to analyze how well it predicts the ratings grouped by the number of tropes.
We calculate the error for every sample of our \textit{extended dataset} and as show it in Figure~\ref{fig:errors}(a) as a hexagonal binning plot,
and we can observe that most of the cases are close to 0 (note the logaritmic scale of the color gradient that helps us see more details).
There are under and over-estimations, especially in the range of tropes with more occurrences \{0-100\},
but the under-estimation error is bigger, that means that the Neural Network will tend to under-estimate for a small number of tropes.

Figure~\ref{fig:errors}(b) and (c) shows the average absolute error and absolute standard deviation by the number of tropes.
Again, we can observe that a small number of tropes (less than 200) implies higher errors and higher deviations than
a bigger number of tropes. That is explained because we are working with user-generated content
and less popular films are poorly described in terms of number of tropes, hence their rating is harder to predict.
At the same time, it points out that the number of tropes is a good predictor,
and, if the films were described with an equal level of detail, it would imply more tropes and less errors.

<<echo=False>>=
mean_1_genre = statistics.mean([item['Estimated rating'] for item in test.test_results['evaluations_1_genre']])
#Out[9]: 5.511383624560505
stdev_1_genre = statistics.stdev([item['Estimated rating'] for item in test.test_results['evaluations_1_genre']])
#Out[10]: 0.010139587722208073
genres = [item['Genre'].replace('[GENRE]','') for item in test.test_results['evaluations_1_genre']]
@

Analysing the weight of the genre on the rating is also a good outcome in order to know if there are rating biases
regarding the genre. We did the evaluation of all the Film DNAs that contain only one genre out of the <%=len(genres)%>,
getting an average value of <%=human_readable(mean_1_genre)%>(+-<%=human_readable(stdev_1_genre)%>), so, in principle,
choosing a single genre or another does not seem to limit the rating. Intuitively, that tells us
that all the genres have good and bad films in the same proportion and our synthetic Film DNA will not suffer
from strong biases due the genre. However, it is the combination of genres what
might boost the rating, and this is precisely where the optimization process
comes into play.
% Añadir tb si hay pelis que tengan más de 10 y menos de 0. Tmabién explicar que puede ser mayor que el rango
% comprobado, no hay ninguna peli con más de 10 o menos de 0. La peli que más tiene es 9.945584884763836


\subsection{The Genetic Algorithm} \label{sec:methodology_ga}

At this step we already have a set of candidate tropes for the Film DNA and a model
that allows its evaluation, so our goal is to use a device that allows us to generate a Film DNA that optimizes the rating.

We chose to use a Genetic Algorithm % TODO cite
because it is a mechanism that allows us to explore the domain of Film DNAs,
in other words, of combinations of tropes, getting high-quality solutions and dealing with global optimization problems.
% cite

Our chromosome will be the a Film DNA, that is, a set of tropes.
In practice it will be encoded as an array of different indices on a dictionary of the total set of tropes available
without value repetitions, given that, \textit{a priori}, the order of the tropes is irrelevant
(according to their nature, they may refer to specific moments, but also to narrative structures and general settings)
and also, our rating evaluator does not consider weights or multiple occurrences of the trope in the movie
(both, training data and model do not consider multiple occurrences of a trope in a single Film, in other words,
the trops appears or does not appear).

The mutation operator changes a trope of the Film DNA by another that is randomly chosen
from the set of tropes out of the Film DNA. By doing this, the Film DNA keeps complying with its restrictions,
mentioned above, and allows an exploration of new tropes.

The crossover operator will make a superset of tropes from the parents' Film DNAs
and randomly selects two subsets for the offspring.
This way, the offspring will have Film DNAs whose tropes are exclusively from their parents, allowing
the exploitation of the data.

As we explained above, we have to rely on a surrogate model for the evaluation of Film DNAs, and our approach uses
a neural network trained with existing movies. The fitness of our GA will be the result of evaluating a DNA trope with
the neural network, that is, their predicted rating.

According to our tests, the GA has proven to converge towards optimal solutions efficiently,
given the simplicity of the operators based on set algebra, and fitness calculation.
It is important to remark that the Genetic Algorithm is interesting for the research  as far as it leads us to our goal,
that is to prove that the methodology works, and hence,
an exploratory analysis of the parameters of the Genetic Algorithm is reasonable in this context,
as we will see in the next section, \ref{sec:experimental_setup}.

%TODO if we are finally using restrictions, we have to explain in (later)

\section{Experimental setup} \label{sec:experimental_setup}

The evaluation of the methodology is performed unequivocally through its testing and a further
analysis of the results. In this research, we want to know if the methodology
can be used to synthesise an optimal Trope DNA for a standard movie in terms of potential rating.

Our experiment will use a fixed size Film DNA of 30 tropes, which is the median of tropes per film
in the original dataset and therefore, a candidate for a 'standard' film.
As benefits, a Film DNA of size 30 is easier to interpret in natural language than one of size 200.
Furthermore, as we saw in the Section~\ref{sec:methodology_mapper}, this number of tropes is a good candidate
because it can lead to a wide range of ratings, good and bad,
and the GA will have a better chance to evolve solution from bad cases;
however, as we saw in the Section~\ref{sec:methodology_surrogate_model}, a Film of size 30 implies
more chances to under or over-evaluate, and the rating will be less accurate.

The most suitable parameters to solve a specific problem require an extensive calibration, which is outside
of the scope of this paper. However, to choose the set of parameters that we will use in all the experiments
in this paper, we have made a preliminary selection executing 30 times every possible combination of Population (P) = \{50, 100, 200\},
Mutation probability (Mp) = \{\(2\div size_{dna}\), \(1\div size_{dna}\), \(0.5\div size_{dna}\)\}
and Crossover Probability (Cp) = \{0.25, 0.5, 0.75\}
and we have chosen the combination P = 200, Mp = \(1\div size_dna\) , Cp = 0.5, because it obtains better results on average.

In the experiments we have set the stop criterion to a minimum of 3000 evaluations and
until the best generation does not improve during 10000 evaluations.
We will run the genetic Algorithm 30 times with different random seeds and
we will select the run that gives the highest fitness in order to analyse it.

As previously stated, a trope is not a full-fledged film, so we need to have an idea of what a movie
with that set of tropes would look like. This is why, as tools to interpret the results,
we will use metrics of similarity between finite sample sets.
The first one is a metric called \textit{Jaccard coefficient}
and is defined as the cardinality of the intersection of Film DNAs divided by the cardinality of the union of the Film DNAs.
The Jaccard coefficient is interesting to measure not only what two sets have in common,
but also, how different their sizes are, penalizing big differences in length of the sets.
However, as we will be comparing our Synthetised Film DNA os a fixed size of 30,
popular films with hundred of tropes will be penalized just because they are too broadly described in comparison;
In order to address this problematic, we will also use a coefficient based only in the common elements
and is defined as the intersection between the sets divided by the length of our synthetic Film DNA.
% TODO Citar las métricas

\section{Results}

The goal of this specific experiment if to find the best Film DNA possible with a fixed small length.

The repetition of the Genetic Algorithm 30 times has resulted in an average fitness of <%=human_readable(9.783487634688873)%>
(+-<%=human_readable(0.4307202616394516)%>).
The best solution has a rating of <%=human_readable(10.313031174658876)%> and a Film DNA with the following tropes.\\


\begin{align*}
& DNA_{Film} = \{ActionHeroBabysitter, DeathByFlashback, DisneyVillainDeath, DuelToTheDeath,\\
& \qquad\qquad\qquad EarlyBirdCameo, FightingFromTheInside, HandsOffParenting, Homage, \\
& \qquad\qquad\qquad ImNotAfraidOfYou, JumpCut, MouthingTheProfanity, NoSympathy, OminousFog, \\
& \qquad\qquad\qquad OneHeadTaller, PoorMansSubstitute, PragmaticAdaptation, \\
& \qquad\qquad\qquad RichIdiotWithNoDayJob, SomeoneToRememberHimBy, SpitefulSpit, TalkingHeads,\\
& \qquad\qquad\qquad TitledAfterTheSong, WeaponOfXSlaying, \\
& \qquad\qquad\qquad [GENRE]Animation, [GENRE]Documentary, [GENRE]Drama, [GENRE]History, \\
& \qquad\qquad\qquad [GENRE]Mystery, [GENRE]Romance, [GENRE]War, [GENRE]Western\}
\end{align*}
\\

<<echo=False>>=
checker = TropesSimilarityChecker()
checker.load_extended_dataset_json(FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE)
tropes_list = ['ActionHeroBabysitter', 'DeathByFlashback', 'DisneyVillainDeath', 'DuelToTheDeath',
               'EarlyBirdCameo', 'FightingFromTheInside', 'HandsOffParenting', 'Homage', 'ImNotAfraidOfYou',
               'JumpCut', 'MouthingTheProfanity', 'NoSympathy', 'OminousFog', 'OneHeadTaller',
               'PoorMansSubstitute', 'PragmaticAdaptation', 'RichIdiotWithNoDayJob', 'SomeoneToRememberHimBy',
               'SpitefulSpit', 'TalkingHeads', 'TitledAfterTheSong', 'WeaponOfXSlaying', '[GENRE]Animation',
               '[GENRE]Documentary', '[GENRE]Drama', '[GENRE]History', '[GENRE]Mystery', '[GENRE]Romance',
               '[GENRE]War', '[GENRE]Western']
top_overlap, top_jaccard, top_common = checker.get_top_films_by_simmilarity(tropes_list, 6)
@

The synthetic Film DNA belongs to a multi-genre film (historical documentary romantic animation drama,
set during a war, that includes western and mistery settings, not comedy).
However, according to one of the tropes, even if it is historical/documentary,
it is completely an adaptation from the author with clear differences with the overall known story
for pragmatic reasons (PragmaticAdaptation).
There are tropes that point to specific features of a hero:
The 'hero' is a rich character with free time (RichIdiotWithNoDayJob),
that has a position of responsibility for children (ActionHeroBabysitter),
that could be related to the existence of irresponsible parents (HandsOffParenting).
There are tropes that define the conflicts and how they will develop:
Someone resisting the controlling influence acting on them ('FightingFromTheInside'),
a villain is finally fought by the hero when he is \textit{not afraid} anymore (ImNotAfraidOfYou),
there is a duel (DuelToTheDeath),
a special weapon is used (WeaponOfXSlaying),
the villain falls off (DisneyVillainDeath),
and the hero also dies, as required by the trope SomeoneToRememberHimBy.
There are tropes that points to a romantic relation between the hero and a girl,
that in some scenes is be clearly taller or shorter than him (OneHeadTaller),
and that discovers, in the end, that is pregnant (SomeoneToRememberHimBy).
There are also tropes that define the setting, in this case, there is fog (OminousFog) as part of the mystery genre
and a clear 'Homage' to a classic or well known artwork in the same genres is present.
According to the narrative perspective,
the story includes a flashback that points to the death of the main character/s (DeathByFlashback),
uses 'Jump Cuts' as an editing technique (JumpCut) and make a character appear before his/her introduction (EarlyBirdCameo).
Some tropes also define very specific sequences: in some cases the film includes spits (SpitefulSpit)
and the characters swear, although it cannot be heard by the audience (MouthingTheProfanity).
There is also a scene where terrible things have happened to the main characters but no-one acts as it is really important (NoSympathy),
and there are scenes with no action, just long conversations where people do not move from their place (TalkingHeads).
The meta-tropes present, on the other hand, include
an actor or actress that is relatively unknown but looks alike another well known one (PoorMansSubstitute)
and the film's name is a reference to an existing song.\\

As part of the analysis, we need to confirm the originality of the Film DNA, so
we perform a similarity analysis against the whole \textit{extended dataset} and we find that the coefficients are small:
According to the Jaccard metric, the most similar films have a value of <%=top_jaccard[0].jaccard %>, in other words,
they are  <%=human_readable(top_jaccard[0].jaccard*100) %>\% similar at maximum.
According to the Common Elements metric, the most similar films have a value of <%=human_readable(top_common[0].common_tropes_count/30)%>
that is higher than the Jaccard coefficient because it is not penalizing the difference of length of the Film DNAs,
in other words, the maximum number of common tropes/genres between the whole \textit{extended dataset} and the
synthesised Film DNA is <%=top_common[0].common_tropes_count %>. There are only 6 films in the whole dataset
with that similarity (<%= print(', '.join([f'\'{film.name}\'' for film in top_common[0:5]]) + f' and \'{top_common[5].name}\'') %>),
presenting all of them the genre Drama and the tropes PragmaticAdaptation, most of them the genre Romance,
and to a lesser extent the genre Mystery and the trope Homage.

%<<echo=False>>=
%    total = []
%    [total.extend(film.common_tropes) for film in top_common[0:6]]
%    from collections import Counter
%    Counter(total)
%@
%[('Arrival',
%  ['Homage',
%   'OminousFog',
%   'PragmaticAdaptation',
%   '[GENRE]Drama',
%   '[GENRE]Mystery']),
% ('Senso',
%  ['DuelToTheDeath',
%   'PragmaticAdaptation',
%   '[GENRE]Drama',
%   '[GENRE]History',
%   '[GENRE]Romance']),
% ('Richard III',
%  ['DisneyVillainDeath',
%   'Homage',
%   'PragmaticAdaptation',
%   '[GENRE]Drama',
%   '[GENRE]War']),
% ('Jane Eyre',
%  ['EarlyBirdCameo',
%   'PragmaticAdaptation',
%   'SpitefulSpit',
%   '[GENRE]Drama',
%   '[GENRE]Romance']),
% ('Lantana',
%  ['HandsOffParenting',
%   'PragmaticAdaptation',
%   '[GENRE]Drama',
%   '[GENRE]Mystery',
%   '[GENRE]Romance']),
% ('Bridget Jones: The Edge of Reason',
%  ['OneHeadTaller',
%   'PragmaticAdaptation',
%   'TitledAfterTheSong',
%   '[GENRE]Drama',
%   '[GENRE]Romance'])]


Our synthesised Film DNA not only has a very high potential rating but also is very different to the rest of the films
in our \textit{extended dataset}.


%If we perform the same analysis with the 30 runs of the experiment, the maximum similarity is ... %TODO

%By looking at the results, the executions lead to Film DNA with an average of X genres and a maximum of T,
%but the extended dataset has an average of Y, with a maximum of Z.
%What is
%we consider that our methodology leads to Film DNAs that promote the
%appearance of more genres than the average: at the same time and that could be hard to implement or, in some cases, incompatible.
%   we changed the fitness function so
%that it punishes solutions with a number of genres different to X, reducing the fitness by 5 points. This technique has been used

%Although the rating is lower than in the previous experiment, a Film with the provided Film DNA is easier to design as we
%are not dealing with big genre incompatibilities.


\section{Conclusions} \label{sec:conclusions}
% TODO: Objetivo ha sido ...
% TODO: Para ello hemos realizado tal (metodología y experimentos) ...
% TODO: Los resultados nos dicen que ...

These results are interesting and directly applicable to the generation of scripts, because every trope or genre
has a clear description, with examples in a dataset of XXX films.
However, putting them together, making them
be part of a coherent story, filling the gaps and narrating them in a compelling way
is still required, not part of the scope of the research and, as far as we know, a human responsibility.


% TODO: Esto puede ser la base para una futura linea...

\section*{Acknowledgements}
This work has been partially funded by projects EphemeCH (TIN2014-56494-C4-3-P), DeepBio (TIN2017-85727-C4-2-P) and TEC2015-68752 and ``Ayuda del Programa de Fomento e Impulso de la actividad Investigadora de la Universidad de C\'adiz''.

\section{Bibliography}
\bibliography{report}

\end{document}
